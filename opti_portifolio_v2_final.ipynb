{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2f7f10b4-7c59-4fbd-840b-8f3225add0d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>^BVSP</th>\n",
       "      <th>ABEV3.SA</th>\n",
       "      <th>ASAI3.SA</th>\n",
       "      <th>AZUL4.SA</th>\n",
       "      <th>B3SA3.SA</th>\n",
       "      <th>BBSE3.SA</th>\n",
       "      <th>BBDC4.SA</th>\n",
       "      <th>BBDC3.SA</th>\n",
       "      <th>BRAP4.SA</th>\n",
       "      <th>...</th>\n",
       "      <th>QUAL3.SA</th>\n",
       "      <th>RADL3.SA</th>\n",
       "      <th>RAIL3.SA</th>\n",
       "      <th>SBSP3.SA</th>\n",
       "      <th>TOTS3.SA</th>\n",
       "      <th>UGPA3.SA</th>\n",
       "      <th>USIM5.SA</th>\n",
       "      <th>VALE3.SA</th>\n",
       "      <th>WEGE3.SA</th>\n",
       "      <th>YDUQ3.SA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-01-02</td>\n",
       "      <td>59589.0</td>\n",
       "      <td>13.968292</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.515615</td>\n",
       "      <td>18.234648</td>\n",
       "      <td>11.856694</td>\n",
       "      <td>12.102320</td>\n",
       "      <td>7.427600</td>\n",
       "      <td>...</td>\n",
       "      <td>14.848257</td>\n",
       "      <td>11.654523</td>\n",
       "      <td>6.11</td>\n",
       "      <td>24.308306</td>\n",
       "      <td>7.483278</td>\n",
       "      <td>29.983891</td>\n",
       "      <td>3.654238</td>\n",
       "      <td>18.692446</td>\n",
       "      <td>5.076362</td>\n",
       "      <td>13.344676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-01-03</td>\n",
       "      <td>61814.0</td>\n",
       "      <td>14.250913</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.701455</td>\n",
       "      <td>18.883545</td>\n",
       "      <td>12.358868</td>\n",
       "      <td>12.592777</td>\n",
       "      <td>7.740072</td>\n",
       "      <td>...</td>\n",
       "      <td>15.495207</td>\n",
       "      <td>12.100480</td>\n",
       "      <td>6.18</td>\n",
       "      <td>24.591063</td>\n",
       "      <td>7.867117</td>\n",
       "      <td>30.469639</td>\n",
       "      <td>3.842324</td>\n",
       "      <td>19.520405</td>\n",
       "      <td>5.331861</td>\n",
       "      <td>14.087488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-01-04</td>\n",
       "      <td>61589.0</td>\n",
       "      <td>14.139578</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.790214</td>\n",
       "      <td>19.480005</td>\n",
       "      <td>12.280593</td>\n",
       "      <td>12.588573</td>\n",
       "      <td>7.571030</td>\n",
       "      <td>...</td>\n",
       "      <td>15.629331</td>\n",
       "      <td>11.929400</td>\n",
       "      <td>6.18</td>\n",
       "      <td>25.302233</td>\n",
       "      <td>7.819534</td>\n",
       "      <td>29.983891</td>\n",
       "      <td>4.075193</td>\n",
       "      <td>19.169830</td>\n",
       "      <td>5.204111</td>\n",
       "      <td>14.018389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-01-05</td>\n",
       "      <td>62071.0</td>\n",
       "      <td>14.216655</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.731965</td>\n",
       "      <td>19.086735</td>\n",
       "      <td>12.416537</td>\n",
       "      <td>12.660000</td>\n",
       "      <td>7.950093</td>\n",
       "      <td>...</td>\n",
       "      <td>15.534655</td>\n",
       "      <td>11.750633</td>\n",
       "      <td>6.19</td>\n",
       "      <td>25.370777</td>\n",
       "      <td>7.860775</td>\n",
       "      <td>30.160522</td>\n",
       "      <td>4.119975</td>\n",
       "      <td>19.900818</td>\n",
       "      <td>5.190664</td>\n",
       "      <td>13.793818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-01-06</td>\n",
       "      <td>61665.0</td>\n",
       "      <td>14.096756</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.729191</td>\n",
       "      <td>18.791780</td>\n",
       "      <td>12.494811</td>\n",
       "      <td>12.756644</td>\n",
       "      <td>7.811786</td>\n",
       "      <td>...</td>\n",
       "      <td>15.376862</td>\n",
       "      <td>11.775620</td>\n",
       "      <td>6.35</td>\n",
       "      <td>25.002340</td>\n",
       "      <td>7.616511</td>\n",
       "      <td>30.028051</td>\n",
       "      <td>4.003540</td>\n",
       "      <td>19.371222</td>\n",
       "      <td>5.234367</td>\n",
       "      <td>13.690170</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 65 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date    ^BVSP   ABEV3.SA  ASAI3.SA  AZUL4.SA  B3SA3.SA   BBSE3.SA  \\\n",
       "0  2017-01-02  59589.0  13.968292       NaN       NaN  4.515615  18.234648   \n",
       "1  2017-01-03  61814.0  14.250913       NaN       NaN  4.701455  18.883545   \n",
       "2  2017-01-04  61589.0  14.139578       NaN       NaN  4.790214  19.480005   \n",
       "3  2017-01-05  62071.0  14.216655       NaN       NaN  4.731965  19.086735   \n",
       "4  2017-01-06  61665.0  14.096756       NaN       NaN  4.729191  18.791780   \n",
       "\n",
       "    BBDC4.SA   BBDC3.SA  BRAP4.SA  ...   QUAL3.SA   RADL3.SA  RAIL3.SA  \\\n",
       "0  11.856694  12.102320  7.427600  ...  14.848257  11.654523      6.11   \n",
       "1  12.358868  12.592777  7.740072  ...  15.495207  12.100480      6.18   \n",
       "2  12.280593  12.588573  7.571030  ...  15.629331  11.929400      6.18   \n",
       "3  12.416537  12.660000  7.950093  ...  15.534655  11.750633      6.19   \n",
       "4  12.494811  12.756644  7.811786  ...  15.376862  11.775620      6.35   \n",
       "\n",
       "    SBSP3.SA  TOTS3.SA   UGPA3.SA  USIM5.SA   VALE3.SA  WEGE3.SA   YDUQ3.SA  \n",
       "0  24.308306  7.483278  29.983891  3.654238  18.692446  5.076362  13.344676  \n",
       "1  24.591063  7.867117  30.469639  3.842324  19.520405  5.331861  14.087488  \n",
       "2  25.302233  7.819534  29.983891  4.075193  19.169830  5.204111  14.018389  \n",
       "3  25.370777  7.860775  30.160522  4.119975  19.900818  5.190664  13.793818  \n",
       "4  25.002340  7.616511  30.028051  4.003540  19.371222  5.234367  13.690170  \n",
       "\n",
       "[5 rows x 65 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "testCSV = './test_market_returns_brl.csv'\n",
    "stockRows = 1243\n",
    "stockCols = 63\n",
    "\n",
    "dfStock = pd.read_csv(testCSV, nrows=stockRows)\n",
    "dfStock.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "77024b69-bf47-46ab-9b2c-90c837abd783",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ABEV3.SA</th>\n",
       "      <th>ASAI3.SA</th>\n",
       "      <th>AZUL4.SA</th>\n",
       "      <th>B3SA3.SA</th>\n",
       "      <th>BBSE3.SA</th>\n",
       "      <th>BBDC4.SA</th>\n",
       "      <th>BBDC3.SA</th>\n",
       "      <th>BRAP4.SA</th>\n",
       "      <th>BBAS3.SA</th>\n",
       "      <th>BRKM5.SA</th>\n",
       "      <th>...</th>\n",
       "      <th>QUAL3.SA</th>\n",
       "      <th>RADL3.SA</th>\n",
       "      <th>RAIL3.SA</th>\n",
       "      <th>SBSP3.SA</th>\n",
       "      <th>TOTS3.SA</th>\n",
       "      <th>UGPA3.SA</th>\n",
       "      <th>USIM5.SA</th>\n",
       "      <th>VALE3.SA</th>\n",
       "      <th>WEGE3.SA</th>\n",
       "      <th>YDUQ3.SA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1033</th>\n",
       "      <td>13.252789</td>\n",
       "      <td>14.074438</td>\n",
       "      <td>40.049999</td>\n",
       "      <td>16.886810</td>\n",
       "      <td>23.280598</td>\n",
       "      <td>17.503073</td>\n",
       "      <td>15.621710</td>\n",
       "      <td>40.217094</td>\n",
       "      <td>25.190319</td>\n",
       "      <td>25.927095</td>\n",
       "      <td>...</td>\n",
       "      <td>29.484564</td>\n",
       "      <td>22.592981</td>\n",
       "      <td>18.080000</td>\n",
       "      <td>34.385944</td>\n",
       "      <td>31.168659</td>\n",
       "      <td>18.085209</td>\n",
       "      <td>15.559167</td>\n",
       "      <td>85.445061</td>\n",
       "      <td>38.487373</td>\n",
       "      <td>28.066465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1034</th>\n",
       "      <td>13.464525</td>\n",
       "      <td>14.291272</td>\n",
       "      <td>38.790001</td>\n",
       "      <td>17.402582</td>\n",
       "      <td>23.271212</td>\n",
       "      <td>17.974874</td>\n",
       "      <td>15.894261</td>\n",
       "      <td>41.219719</td>\n",
       "      <td>26.157789</td>\n",
       "      <td>24.890991</td>\n",
       "      <td>...</td>\n",
       "      <td>29.938887</td>\n",
       "      <td>22.692160</td>\n",
       "      <td>18.299999</td>\n",
       "      <td>35.220600</td>\n",
       "      <td>30.713209</td>\n",
       "      <td>18.027338</td>\n",
       "      <td>15.910055</td>\n",
       "      <td>88.071594</td>\n",
       "      <td>38.023544</td>\n",
       "      <td>27.245867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1035</th>\n",
       "      <td>13.637765</td>\n",
       "      <td>14.054728</td>\n",
       "      <td>38.560001</td>\n",
       "      <td>17.582790</td>\n",
       "      <td>23.111626</td>\n",
       "      <td>18.132408</td>\n",
       "      <td>16.216074</td>\n",
       "      <td>40.366554</td>\n",
       "      <td>26.212038</td>\n",
       "      <td>24.075159</td>\n",
       "      <td>...</td>\n",
       "      <td>30.495197</td>\n",
       "      <td>23.019449</td>\n",
       "      <td>18.490000</td>\n",
       "      <td>35.890270</td>\n",
       "      <td>30.624098</td>\n",
       "      <td>17.574001</td>\n",
       "      <td>15.993162</td>\n",
       "      <td>86.988045</td>\n",
       "      <td>37.993942</td>\n",
       "      <td>27.040718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1036</th>\n",
       "      <td>14.224853</td>\n",
       "      <td>14.429256</td>\n",
       "      <td>38.500000</td>\n",
       "      <td>17.241013</td>\n",
       "      <td>22.998978</td>\n",
       "      <td>18.833441</td>\n",
       "      <td>16.757658</td>\n",
       "      <td>39.114830</td>\n",
       "      <td>27.161425</td>\n",
       "      <td>24.580973</td>\n",
       "      <td>...</td>\n",
       "      <td>31.292580</td>\n",
       "      <td>23.406250</td>\n",
       "      <td>19.850000</td>\n",
       "      <td>37.161663</td>\n",
       "      <td>30.307264</td>\n",
       "      <td>18.663937</td>\n",
       "      <td>15.522230</td>\n",
       "      <td>85.696442</td>\n",
       "      <td>36.133724</td>\n",
       "      <td>27.040718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1037</th>\n",
       "      <td>14.513584</td>\n",
       "      <td>14.784074</td>\n",
       "      <td>38.139999</td>\n",
       "      <td>17.551720</td>\n",
       "      <td>23.609154</td>\n",
       "      <td>19.723518</td>\n",
       "      <td>17.675995</td>\n",
       "      <td>41.599594</td>\n",
       "      <td>27.667765</td>\n",
       "      <td>24.907307</td>\n",
       "      <td>...</td>\n",
       "      <td>31.783987</td>\n",
       "      <td>23.525265</td>\n",
       "      <td>20.309999</td>\n",
       "      <td>36.394947</td>\n",
       "      <td>29.822111</td>\n",
       "      <td>18.979773</td>\n",
       "      <td>16.427158</td>\n",
       "      <td>90.780022</td>\n",
       "      <td>36.039967</td>\n",
       "      <td>28.086006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1238</th>\n",
       "      <td>15.550000</td>\n",
       "      <td>13.976556</td>\n",
       "      <td>26.459999</td>\n",
       "      <td>11.191103</td>\n",
       "      <td>19.832745</td>\n",
       "      <td>17.376770</td>\n",
       "      <td>14.667024</td>\n",
       "      <td>22.717564</td>\n",
       "      <td>28.033241</td>\n",
       "      <td>52.768032</td>\n",
       "      <td>...</td>\n",
       "      <td>15.870000</td>\n",
       "      <td>24.100000</td>\n",
       "      <td>17.670000</td>\n",
       "      <td>39.077812</td>\n",
       "      <td>28.608072</td>\n",
       "      <td>14.270000</td>\n",
       "      <td>14.860000</td>\n",
       "      <td>79.150002</td>\n",
       "      <td>33.239605</td>\n",
       "      <td>20.177418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1239</th>\n",
       "      <td>15.530000</td>\n",
       "      <td>13.530179</td>\n",
       "      <td>25.670000</td>\n",
       "      <td>11.122266</td>\n",
       "      <td>19.957781</td>\n",
       "      <td>17.575462</td>\n",
       "      <td>14.820463</td>\n",
       "      <td>23.323111</td>\n",
       "      <td>28.090803</td>\n",
       "      <td>52.887875</td>\n",
       "      <td>...</td>\n",
       "      <td>16.650000</td>\n",
       "      <td>24.070000</td>\n",
       "      <td>17.799999</td>\n",
       "      <td>39.234554</td>\n",
       "      <td>28.677750</td>\n",
       "      <td>14.560000</td>\n",
       "      <td>15.040000</td>\n",
       "      <td>78.949997</td>\n",
       "      <td>32.941666</td>\n",
       "      <td>19.919369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1240</th>\n",
       "      <td>15.520000</td>\n",
       "      <td>12.994527</td>\n",
       "      <td>25.750000</td>\n",
       "      <td>10.974755</td>\n",
       "      <td>19.996254</td>\n",
       "      <td>17.512243</td>\n",
       "      <td>14.811438</td>\n",
       "      <td>23.181187</td>\n",
       "      <td>27.927708</td>\n",
       "      <td>52.814129</td>\n",
       "      <td>...</td>\n",
       "      <td>16.820000</td>\n",
       "      <td>23.709999</td>\n",
       "      <td>17.530001</td>\n",
       "      <td>39.224758</td>\n",
       "      <td>28.190001</td>\n",
       "      <td>14.610000</td>\n",
       "      <td>14.750000</td>\n",
       "      <td>77.050003</td>\n",
       "      <td>33.060844</td>\n",
       "      <td>20.703440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1241</th>\n",
       "      <td>15.450000</td>\n",
       "      <td>12.736620</td>\n",
       "      <td>23.860001</td>\n",
       "      <td>10.896083</td>\n",
       "      <td>19.880835</td>\n",
       "      <td>17.430958</td>\n",
       "      <td>14.703128</td>\n",
       "      <td>23.171724</td>\n",
       "      <td>27.783798</td>\n",
       "      <td>52.814129</td>\n",
       "      <td>...</td>\n",
       "      <td>16.020000</td>\n",
       "      <td>23.780001</td>\n",
       "      <td>17.500000</td>\n",
       "      <td>39.038624</td>\n",
       "      <td>27.889999</td>\n",
       "      <td>14.400000</td>\n",
       "      <td>14.800000</td>\n",
       "      <td>77.250000</td>\n",
       "      <td>32.514629</td>\n",
       "      <td>20.167492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1242</th>\n",
       "      <td>15.420000</td>\n",
       "      <td>12.855654</td>\n",
       "      <td>24.360001</td>\n",
       "      <td>10.955088</td>\n",
       "      <td>19.957781</td>\n",
       "      <td>17.349674</td>\n",
       "      <td>14.612867</td>\n",
       "      <td>23.644810</td>\n",
       "      <td>27.678268</td>\n",
       "      <td>53.127563</td>\n",
       "      <td>...</td>\n",
       "      <td>16.900000</td>\n",
       "      <td>24.299999</td>\n",
       "      <td>17.760000</td>\n",
       "      <td>39.361908</td>\n",
       "      <td>28.639999</td>\n",
       "      <td>14.540000</td>\n",
       "      <td>15.160000</td>\n",
       "      <td>77.959999</td>\n",
       "      <td>32.752975</td>\n",
       "      <td>20.405691</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>210 rows Ã— 63 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ABEV3.SA   ASAI3.SA   AZUL4.SA   B3SA3.SA   BBSE3.SA   BBDC4.SA  \\\n",
       "1033  13.252789  14.074438  40.049999  16.886810  23.280598  17.503073   \n",
       "1034  13.464525  14.291272  38.790001  17.402582  23.271212  17.974874   \n",
       "1035  13.637765  14.054728  38.560001  17.582790  23.111626  18.132408   \n",
       "1036  14.224853  14.429256  38.500000  17.241013  22.998978  18.833441   \n",
       "1037  14.513584  14.784074  38.139999  17.551720  23.609154  19.723518   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "1238  15.550000  13.976556  26.459999  11.191103  19.832745  17.376770   \n",
       "1239  15.530000  13.530179  25.670000  11.122266  19.957781  17.575462   \n",
       "1240  15.520000  12.994527  25.750000  10.974755  19.996254  17.512243   \n",
       "1241  15.450000  12.736620  23.860001  10.896083  19.880835  17.430958   \n",
       "1242  15.420000  12.855654  24.360001  10.955088  19.957781  17.349674   \n",
       "\n",
       "       BBDC3.SA   BRAP4.SA   BBAS3.SA   BRKM5.SA  ...   QUAL3.SA   RADL3.SA  \\\n",
       "1033  15.621710  40.217094  25.190319  25.927095  ...  29.484564  22.592981   \n",
       "1034  15.894261  41.219719  26.157789  24.890991  ...  29.938887  22.692160   \n",
       "1035  16.216074  40.366554  26.212038  24.075159  ...  30.495197  23.019449   \n",
       "1036  16.757658  39.114830  27.161425  24.580973  ...  31.292580  23.406250   \n",
       "1037  17.675995  41.599594  27.667765  24.907307  ...  31.783987  23.525265   \n",
       "...         ...        ...        ...        ...  ...        ...        ...   \n",
       "1238  14.667024  22.717564  28.033241  52.768032  ...  15.870000  24.100000   \n",
       "1239  14.820463  23.323111  28.090803  52.887875  ...  16.650000  24.070000   \n",
       "1240  14.811438  23.181187  27.927708  52.814129  ...  16.820000  23.709999   \n",
       "1241  14.703128  23.171724  27.783798  52.814129  ...  16.020000  23.780001   \n",
       "1242  14.612867  23.644810  27.678268  53.127563  ...  16.900000  24.299999   \n",
       "\n",
       "       RAIL3.SA   SBSP3.SA   TOTS3.SA   UGPA3.SA   USIM5.SA   VALE3.SA  \\\n",
       "1033  18.080000  34.385944  31.168659  18.085209  15.559167  85.445061   \n",
       "1034  18.299999  35.220600  30.713209  18.027338  15.910055  88.071594   \n",
       "1035  18.490000  35.890270  30.624098  17.574001  15.993162  86.988045   \n",
       "1036  19.850000  37.161663  30.307264  18.663937  15.522230  85.696442   \n",
       "1037  20.309999  36.394947  29.822111  18.979773  16.427158  90.780022   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "1238  17.670000  39.077812  28.608072  14.270000  14.860000  79.150002   \n",
       "1239  17.799999  39.234554  28.677750  14.560000  15.040000  78.949997   \n",
       "1240  17.530001  39.224758  28.190001  14.610000  14.750000  77.050003   \n",
       "1241  17.500000  39.038624  27.889999  14.400000  14.800000  77.250000   \n",
       "1242  17.760000  39.361908  28.639999  14.540000  15.160000  77.959999   \n",
       "\n",
       "       WEGE3.SA   YDUQ3.SA  \n",
       "1033  38.487373  28.066465  \n",
       "1034  38.023544  27.245867  \n",
       "1035  37.993942  27.040718  \n",
       "1036  36.133724  27.040718  \n",
       "1037  36.039967  28.086006  \n",
       "...         ...        ...  \n",
       "1238  33.239605  20.177418  \n",
       "1239  32.941666  19.919369  \n",
       "1240  33.060844  20.703440  \n",
       "1241  32.514629  20.167492  \n",
       "1242  32.752975  20.405691  \n",
       "\n",
       "[210 rows x 63 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cleaning the data\n",
    "dfStock = dfStock.dropna(axis=0)\n",
    "stockData = dfStock.iloc[0:, 2:]\n",
    "stockData\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "386a529e-6d54-440d-891f-ca2a75748995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retorna um array com compound return de cada acao\n",
    "def getCompoundReturnLog(dataPrices):   \n",
    "    returns = np.log(dataPrices/dataPrices.shift(1)).dropna(how=\"any\")\n",
    "    return np.asarray(returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "d9b21021-d37e-4879-890f-89b400445857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ABEV3.SA', 'ASAI3.SA', 'AZUL4.SA', 'B3SA3.SA', 'BBSE3.SA', 'BBDC4.SA', 'BBDC3.SA', 'BRAP4.SA', 'BBAS3.SA', 'BRKM5.SA', 'BRFS3.SA', 'CRFB3.SA', 'CCRO3.SA', 'COGN3.SA', 'CPLE6.SA', 'CSAN3.SA', 'CPFE3.SA', 'CVCB3.SA', 'CYRE3.SA', 'ECOR3.SA', 'ELET3.SA', 'ELET6.SA', 'EMBR3.SA', 'ENBR3.SA', 'ENGI11.SA', 'ENEV3.SA', 'EQTL3.SA', 'EZTC3.SA', 'FLRY3.SA', 'GGBR4.SA', 'GOAU4.SA', 'GOLL4.SA', 'NTCO3.SA', 'HAPV3.SA', 'HYPE3.SA', 'IRBR3.SA', 'ITSA4.SA', 'ITUB4.SA', 'JBSS3.SA', 'JHSF3.SA', 'KLBN11.SA', 'RENT3.SA', 'LCAM3.SA', 'LREN3.SA', 'MGLU3.SA', 'MRFG3.SA', 'BEEF3.SA', 'MRVE3.SA', 'MULT3.SA', 'PCAR3.SA', 'PETR3.SA', 'PETR4.SA', 'PRIO3.SA', 'QUAL3.SA', 'RADL3.SA', 'RAIL3.SA', 'SBSP3.SA', 'TOTS3.SA', 'UGPA3.SA', 'USIM5.SA', 'VALE3.SA', 'WEGE3.SA']\n",
      "Media dos retornos:\n",
      " [ 0.072 -0.043 -0.238 -0.207 -0.074 -0.004 -0.032 -0.254  0.045  0.343\n",
      "  0.018 -0.079  0.035 -0.184  0.126  0.038  0.042 -0.083 -0.189 -0.179\n",
      "  0.04   0.029  0.339  0.102  0.033 -0.08   0.076 -0.186 -0.158  0.066\n",
      "  0.075 -0.109 -0.287 -0.215 -0.033 -0.178  0.006 -0.068  0.223 -0.031\n",
      " -0.08  -0.049 -0.005 -0.145 -0.585  0.295  0.095 -0.139 -0.004 -0.007\n",
      "  0.256  0.222  0.076 -0.266  0.035 -0.009  0.065 -0.04  -0.104 -0.012\n",
      " -0.044 -0.077 -0.153]\n",
      "Matrix de Covariancia dos retornos:\n",
      " [[ 3.36   0.875  2.102 ...  0.233  0.773  1.853]\n",
      " [ 0.875  3.58   2.117 ...  0.377  1.13   1.774]\n",
      " [ 2.102  2.117 11.59  ...  1.778  2.292  4.788]\n",
      " ...\n",
      " [ 0.233  0.377  1.778 ...  4.686  0.187  0.976]\n",
      " [ 0.773  1.13   2.292 ...  0.187  3.89   2.224]\n",
      " [ 1.853  1.774  4.788 ...  0.976  2.224  9.086]]\n"
     ]
    }
   ],
   "source": [
    "# pegando as labels das stocks\n",
    "stockLabels = dfStock.columns[2:stockCols + 1].tolist()\n",
    "\n",
    "# retornos das stocks em %\n",
    "stockReturnsArr = getCompoundReturnLog(stockData)*100\n",
    "\n",
    "# media dos retornos\n",
    "print(stockLabels)\n",
    "meanReturns = np.mean(stockReturnsArr, axis = 0)\n",
    "print('Media dos retornos:\\n', meanReturns)\n",
    "\n",
    "# matriz de covariancia entre as stocks\n",
    "covReturns = np.cov(stockReturnsArr, rowvar = False)\n",
    "print('Matrix de Covariancia dos retornos:\\n', covReturns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "205da28f-e3d5-4d76-be8a-2d22418041ac",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Version: 0.0.4 tol**-3 *****\n",
      "     fun: -0.09025853986817758\n",
      "     jac: array([-0.072,  0.043,  0.238,  0.207,  0.074,  0.004,  0.032,  0.254,\n",
      "       -0.045, -0.343, -0.018,  0.079, -0.035,  0.184, -0.126, -0.038,\n",
      "       -0.042,  0.083,  0.189,  0.179, -0.04 , -0.029, -0.339, -0.102,\n",
      "       -0.033,  0.08 , -0.076,  0.186,  0.158, -0.066, -0.075,  0.109,\n",
      "        0.287,  0.215,  0.033,  0.178, -0.006,  0.068, -0.223,  0.031,\n",
      "        0.08 ,  0.049,  0.005,  0.145,  0.585, -0.295, -0.095,  0.139,\n",
      "        0.004,  0.007, -0.256, -0.222, -0.076,  0.266, -0.035,  0.009,\n",
      "       -0.065,  0.04 ,  0.104,  0.012,  0.044,  0.077,  0.153])\n",
      " message: 'Optimization terminated successfully'\n",
      "    nfev: 192\n",
      "     nit: 3\n",
      "    njev: 3\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([0.045, 0.   , 0.01 , 0.01 , 0.   , 0.01 , 0.01 , 0.   , 0.01 ,\n",
      "       0.01 , 0.   , 0.   , 0.   , 0.01 , 0.1  , 0.   , 0.   , 0.01 ,\n",
      "       0.01 , 0.01 , 0.01 , 0.01 , 0.01 , 0.1  , 0.   , 0.   , 0.052,\n",
      "       0.01 , 0.   , 0.033, 0.05 , 0.01 , 0.   , 0.   , 0.   , 0.   ,\n",
      "       0.   , 0.01 , 0.1  , 0.01 , 0.   , 0.01 , 0.01 , 0.01 , 0.   ,\n",
      "       0.1  , 0.091, 0.01 , 0.01 , 0.01 , 0.01 , 0.01 , 0.01 , 0.   ,\n",
      "       0.   , 0.01 , 0.029, 0.   , 0.01 , 0.01 , 0.   , 0.   , 0.01 ])\n",
      "***** Version: 0.0.4 tol**-3 *****\n",
      "     fun: -0.05917979821519247\n",
      "     jac: array([-0.02 ,  0.083,  0.33 ,  0.269,  0.116,  0.058,  0.087,  0.273,\n",
      "        0.01 , -0.267,  0.038,  0.122,  0.024,  0.271, -0.067,  0.018,\n",
      "        0.   ,  0.191,  0.277,  0.248,  0.024,  0.034, -0.247, -0.064,\n",
      "        0.017,  0.136, -0.024,  0.277,  0.2  , -0.012, -0.022,  0.198,\n",
      "        0.337,  0.256,  0.082,  0.226,  0.043,  0.121, -0.164,  0.119,\n",
      "        0.1  ,  0.113,  0.074,  0.208,  0.648, -0.239, -0.024,  0.219,\n",
      "        0.079,  0.073, -0.197, -0.16 , -0.009,  0.312,  0.002,  0.065,\n",
      "       -0.008,  0.096,  0.187,  0.084,  0.068,  0.118,  0.225])\n",
      " message: 'Optimization terminated successfully'\n",
      "    nfev: 192\n",
      "     nit: 3\n",
      "    njev: 3\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([0.05 , 0.   , 0.01 , 0.01 , 0.   , 0.01 , 0.01 , 0.   , 0.01 ,\n",
      "       0.01 , 0.   , 0.   , 0.   , 0.01 , 0.1  , 0.   , 0.   , 0.01 ,\n",
      "       0.01 , 0.01 , 0.01 , 0.01 , 0.01 , 0.1  , 0.   , 0.   , 0.053,\n",
      "       0.01 , 0.   , 0.035, 0.054, 0.01 , 0.   , 0.   , 0.   , 0.   ,\n",
      "       0.   , 0.01 , 0.1  , 0.01 , 0.   , 0.01 , 0.01 , 0.01 , 0.   ,\n",
      "       0.1  , 0.085, 0.01 , 0.01 , 0.01 , 0.01 , 0.01 , 0.01 , 0.   ,\n",
      "       0.   , 0.01 , 0.023, 0.   , 0.01 , 0.01 , 0.   , 0.   , 0.01 ])\n",
      "***** Version: 0.0.4 tol**-3 *****\n",
      "     fun: -0.02826700023386302\n",
      "     jac: array([ 0.032,  0.122,  0.422,  0.331,  0.158,  0.113,  0.143,  0.291,\n",
      "        0.064, -0.191,  0.092,  0.164,  0.083,  0.358, -0.009,  0.074,\n",
      "        0.044,  0.298,  0.365,  0.317,  0.088,  0.096, -0.155, -0.026,\n",
      "        0.068,  0.192,  0.028,  0.368,  0.244,  0.042,  0.031,  0.287,\n",
      "        0.387,  0.297,  0.131,  0.273,  0.091,  0.174, -0.107,  0.207,\n",
      "        0.12 ,  0.177,  0.144,  0.272,  0.711, -0.186,  0.04 ,  0.3  ,\n",
      "        0.154,  0.138, -0.138, -0.098,  0.056,  0.358,  0.041,  0.121,\n",
      "        0.047,  0.152,  0.269,  0.155,  0.093,  0.158,  0.297])\n",
      " message: 'Optimization terminated successfully'\n",
      "    nfev: 192\n",
      "     nit: 3\n",
      "    njev: 3\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([0.052, 0.   , 0.01 , 0.01 , 0.   , 0.01 , 0.01 , 0.   , 0.01 ,\n",
      "       0.01 , 0.   , 0.   , 0.   , 0.01 , 0.1  , 0.   , 0.011, 0.01 ,\n",
      "       0.01 , 0.01 , 0.01 , 0.01 , 0.01 , 0.1  , 0.   , 0.   , 0.052,\n",
      "       0.01 , 0.   , 0.035, 0.055, 0.01 , 0.   , 0.   , 0.   , 0.   ,\n",
      "       0.   , 0.01 , 0.1  , 0.01 , 0.   , 0.01 , 0.01 , 0.01 , 0.   ,\n",
      "       0.1  , 0.073, 0.01 , 0.01 , 0.01 , 0.01 , 0.01 , 0.01 , 0.   ,\n",
      "       0.008, 0.01 , 0.014, 0.   , 0.01 , 0.01 , 0.   , 0.   , 0.01 ])\n",
      "***** Version: 0.0.4 tol**-3 *****\n",
      "     fun: 0.001866082461224411\n",
      "     jac: array([ 0.084,  0.16 ,  0.514,  0.393,  0.2  ,  0.168,  0.198,  0.31 ,\n",
      "        0.119, -0.117,  0.144,  0.207,  0.142,  0.447,  0.048,  0.129,\n",
      "        0.089,  0.406,  0.452,  0.387,  0.153,  0.158, -0.064,  0.013,\n",
      "        0.12 ,  0.249,  0.081,  0.459,  0.287,  0.095,  0.082,  0.375,\n",
      "        0.438,  0.34 ,  0.181,  0.321,  0.14 ,  0.227, -0.053,  0.294,\n",
      "        0.14 ,  0.242,  0.214,  0.337,  0.775, -0.135,  0.098,  0.38 ,\n",
      "        0.23 ,  0.204, -0.079, -0.037,  0.119,  0.407,  0.084,  0.177,\n",
      "        0.101,  0.208,  0.351,  0.224,  0.117,  0.199,  0.37 ])\n",
      " message: 'Optimization terminated successfully'\n",
      "    nfev: 192\n",
      "     nit: 3\n",
      "    njev: 3\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([0.051, 0.   , 0.01 , 0.01 , 0.   , 0.01 , 0.01 , 0.   , 0.01 ,\n",
      "       0.01 , 0.   , 0.   , 0.   , 0.01 , 0.1  , 0.   , 0.021, 0.01 ,\n",
      "       0.01 , 0.01 , 0.01 , 0.01 , 0.01 , 0.1  , 0.   , 0.   , 0.049,\n",
      "       0.01 , 0.   , 0.034, 0.056, 0.01 , 0.   , 0.   , 0.   , 0.   ,\n",
      "       0.   , 0.01 , 0.1  , 0.01 , 0.   , 0.01 , 0.01 , 0.01 , 0.   ,\n",
      "       0.1  , 0.06 , 0.01 , 0.01 , 0.01 , 0.01 , 0.01 , 0.01 , 0.   ,\n",
      "       0.022, 0.01 , 0.006, 0.   , 0.01 , 0.01 , 0.   , 0.   , 0.01 ])\n",
      "***** Version: 0.0.4 tol**-3 *****\n",
      "     fun: 0.03137594718555102\n",
      "     jac: array([ 0.136,  0.199,  0.605,  0.457,  0.241,  0.223,  0.255,  0.329,\n",
      "        0.174, -0.043,  0.193,  0.249,  0.201,  0.537,  0.105,  0.184,\n",
      "        0.135,  0.514,  0.54 ,  0.457,  0.218,  0.222,  0.028,  0.052,\n",
      "        0.172,  0.306,  0.134,  0.55 ,  0.331,  0.147,  0.133,  0.463,\n",
      "        0.489,  0.384,  0.23 ,  0.369,  0.188,  0.281, -0.002,  0.382,\n",
      "        0.159,  0.307,  0.285,  0.402,  0.839, -0.087,  0.148,  0.46 ,\n",
      "        0.305,  0.269, -0.019,  0.025,  0.181,  0.457,  0.13 ,  0.233,\n",
      "        0.154,  0.266,  0.432,  0.294,  0.142,  0.241,  0.444])\n",
      " message: 'Optimization terminated successfully'\n",
      "    nfev: 192\n",
      "     nit: 3\n",
      "    njev: 3\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([0.051, 0.   , 0.01 , 0.01 , 0.   , 0.01 , 0.01 , 0.   , 0.01 ,\n",
      "       0.01 , 0.   , 0.   , 0.   , 0.01 , 0.1  , 0.   , 0.031, 0.01 ,\n",
      "       0.01 , 0.01 , 0.01 , 0.01 , 0.01 , 0.1  , 0.   , 0.   , 0.047,\n",
      "       0.01 , 0.   , 0.035, 0.057, 0.01 , 0.   , 0.   , 0.   , 0.   ,\n",
      "       0.   , 0.01 , 0.1  , 0.01 , 0.   , 0.01 , 0.01 , 0.01 , 0.   ,\n",
      "       0.1  , 0.045, 0.01 , 0.01 , 0.01 , 0.01 , 0.01 , 0.01 , 0.   ,\n",
      "       0.035, 0.01 , 0.   , 0.   , 0.01 , 0.01 , 0.   , 0.   , 0.01 ])\n",
      "***** Version: 0.0.4 tol**-3 *****\n",
      "     fun: 0.060351404153939206\n",
      "     jac: array([ 0.183,  0.235,  0.697,  0.517,  0.28 ,  0.278,  0.31 ,  0.353,\n",
      "        0.227,  0.028,  0.237,  0.291,  0.257,  0.624,  0.158,  0.236,\n",
      "        0.178,  0.616,  0.625,  0.524,  0.283,  0.283,  0.117,  0.089,\n",
      "        0.222,  0.359,  0.184,  0.636,  0.371,  0.203,  0.187,  0.551,\n",
      "        0.537,  0.426,  0.276,  0.414,  0.234,  0.334,  0.043,  0.464,\n",
      "        0.181,  0.37 ,  0.352,  0.463,  0.9  , -0.042,  0.186,  0.537,\n",
      "        0.377,  0.33 ,  0.039,  0.086,  0.243,  0.507,  0.172,  0.287,\n",
      "        0.206,  0.319,  0.508,  0.372,  0.181,  0.279,  0.517])\n",
      " message: 'Optimization terminated successfully'\n",
      "    nfev: 192\n",
      "     nit: 3\n",
      "    njev: 3\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([0.046, 0.   , 0.01 , 0.01 , 0.   , 0.01 , 0.01 , 0.   , 0.01 ,\n",
      "       0.01 , 0.   , 0.   , 0.   , 0.01 , 0.1  , 0.   , 0.035, 0.01 ,\n",
      "       0.01 , 0.01 , 0.01 , 0.01 , 0.01 , 0.1  , 0.   , 0.   , 0.041,\n",
      "       0.01 , 0.   , 0.03 , 0.052, 0.01 , 0.   , 0.   , 0.   , 0.   ,\n",
      "       0.   , 0.01 , 0.1  , 0.01 , 0.002, 0.01 , 0.01 , 0.01 , 0.   ,\n",
      "       0.1  , 0.029, 0.01 , 0.01 , 0.01 , 0.01 , 0.01 , 0.01 , 0.   ,\n",
      "       0.041, 0.01 , 0.   , 0.   , 0.01 , 0.01 , 0.022, 0.   , 0.01 ])\n",
      "***** Version: 0.0.4 tol**-3 *****\n",
      "     fun: 0.08791432964679682\n",
      "     jac: array([0.225, 0.269, 0.782, 0.574, 0.315, 0.328, 0.361, 0.367, 0.276,\n",
      "       0.095, 0.276, 0.33 , 0.309, 0.705, 0.205, 0.283, 0.221, 0.71 ,\n",
      "       0.704, 0.586, 0.342, 0.339, 0.199, 0.125, 0.27 , 0.41 , 0.228,\n",
      "       0.718, 0.408, 0.246, 0.229, 0.633, 0.585, 0.468, 0.319, 0.457,\n",
      "       0.275, 0.38 , 0.085, 0.539, 0.215, 0.429, 0.416, 0.518, 0.959,\n",
      "       0.001, 0.216, 0.607, 0.443, 0.388, 0.091, 0.14 , 0.3  , 0.558,\n",
      "       0.211, 0.336, 0.253, 0.371, 0.579, 0.442, 0.218, 0.315, 0.587])\n",
      " message: 'Optimization terminated successfully'\n",
      "    nfev: 192\n",
      "     nit: 3\n",
      "    njev: 3\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([0.041, 0.   , 0.01 , 0.01 , 0.   , 0.01 , 0.01 , 0.   , 0.01 ,\n",
      "       0.01 , 0.   , 0.   , 0.   , 0.01 , 0.1  , 0.   , 0.038, 0.01 ,\n",
      "       0.01 , 0.01 , 0.01 , 0.01 , 0.01 , 0.1  , 0.   , 0.   , 0.037,\n",
      "       0.01 , 0.   , 0.018, 0.041, 0.01 , 0.   , 0.   , 0.   , 0.   ,\n",
      "       0.   , 0.01 , 0.1  , 0.01 , 0.028, 0.01 , 0.01 , 0.01 , 0.   ,\n",
      "       0.1  , 0.014, 0.01 , 0.01 , 0.01 , 0.01 , 0.01 , 0.01 , 0.   ,\n",
      "       0.045, 0.01 , 0.   , 0.   , 0.01 , 0.01 , 0.039, 0.   , 0.01 ])\n",
      "***** Version: 0.0.4 tol**-3 *****\n",
      "     fun: 0.12248861312881237\n",
      "     jac: array([ 0.178,  0.276,  0.781,  0.57 ,  0.306,  0.328,  0.361,  0.37 ,\n",
      "        0.262,  0.055,  0.225,  0.318,  0.299,  0.735,  0.127,  0.284,\n",
      "        0.197,  0.739,  0.731,  0.598,  0.344,  0.336,  0.195,  0.104,\n",
      "        0.251,  0.406,  0.2  ,  0.741,  0.408,  0.19 ,  0.179,  0.65 ,\n",
      "        0.582,  0.493,  0.311,  0.454,  0.262,  0.377, -0.058,  0.538,\n",
      "        0.181,  0.455,  0.445,  0.537,  1.026, -0.173,  0.122,  0.611,\n",
      "        0.439,  0.354,  0.09 ,  0.138,  0.302,  0.565,  0.194,  0.334,\n",
      "        0.225,  0.369,  0.547,  0.382,  0.19 ,  0.302,  0.621])\n",
      " message: 'Optimization terminated successfully'\n",
      "    nfev: 65\n",
      "     nit: 1\n",
      "    njev: 1\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([0.038, 0.   , 0.01 , 0.01 , 0.   , 0.01 , 0.01 , 0.   , 0.01 ,\n",
      "       0.01 , 0.   , 0.   , 0.   , 0.01 , 0.089, 0.   , 0.019, 0.01 ,\n",
      "       0.01 , 0.01 , 0.01 , 0.01 , 0.01 , 0.1  , 0.   , 0.   , 0.016,\n",
      "       0.01 , 0.   , 0.026, 0.036, 0.01 , 0.   , 0.   , 0.   , 0.   ,\n",
      "       0.   , 0.01 , 0.1  , 0.01 , 0.035, 0.01 , 0.01 , 0.01 , 0.   ,\n",
      "       0.1  , 0.093, 0.01 , 0.01 , 0.01 , 0.01 , 0.01 , 0.01 , 0.   ,\n",
      "       0.022, 0.01 , 0.   , 0.   , 0.01 , 0.01 , 0.026, 0.   , 0.01 ])\n",
      "***** Version: 0.0.4 tol**-3 *****\n",
      "     fun: 0.14000534355234412\n",
      "     jac: array([0.302, 0.336, 0.947, 0.683, 0.382, 0.423, 0.457, 0.388, 0.37 ,\n",
      "       0.228, 0.353, 0.409, 0.408, 0.859, 0.294, 0.372, 0.302, 0.89 ,\n",
      "       0.854, 0.702, 0.453, 0.445, 0.358, 0.196, 0.361, 0.51 , 0.308,\n",
      "       0.875, 0.481, 0.32 , 0.302, 0.793, 0.679, 0.552, 0.403, 0.541,\n",
      "       0.353, 0.467, 0.173, 0.683, 0.287, 0.544, 0.539, 0.623, 1.078,\n",
      "       0.091, 0.285, 0.742, 0.572, 0.504, 0.188, 0.241, 0.413, 0.659,\n",
      "       0.291, 0.43 , 0.343, 0.474, 0.717, 0.573, 0.288, 0.387, 0.723])\n",
      " message: 'Optimization terminated successfully'\n",
      "    nfev: 192\n",
      "     nit: 3\n",
      "    njev: 3\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([0.031, 0.   , 0.01 , 0.01 , 0.   , 0.01 , 0.01 , 0.   , 0.01 ,\n",
      "       0.01 , 0.   , 0.   , 0.   , 0.01 , 0.1  , 0.   , 0.041, 0.01 ,\n",
      "       0.01 , 0.01 , 0.01 , 0.01 , 0.01 , 0.1  , 0.   , 0.   , 0.027,\n",
      "       0.01 , 0.   , 0.   , 0.017, 0.01 , 0.   , 0.   , 0.   , 0.   ,\n",
      "       0.   , 0.01 , 0.1  , 0.01 , 0.069, 0.01 , 0.01 , 0.01 , 0.   ,\n",
      "       0.1  , 0.   , 0.01 , 0.01 , 0.01 , 0.01 , 0.01 , 0.01 , 0.   ,\n",
      "       0.052, 0.01 , 0.   , 0.   , 0.01 , 0.01 , 0.064, 0.   , 0.01 ])\n",
      "***** Version: 0.0.4 tol**-3 *****\n",
      "     fun: 0.16532657808765214\n",
      "     jac: array([0.339, 0.369, 1.028, 0.736, 0.415, 0.468, 0.503, 0.398, 0.415,\n",
      "       0.296, 0.394, 0.448, 0.457, 0.935, 0.335, 0.417, 0.341, 0.979,\n",
      "       0.927, 0.759, 0.505, 0.495, 0.437, 0.229, 0.405, 0.559, 0.346,\n",
      "       0.952, 0.517, 0.358, 0.339, 0.873, 0.726, 0.593, 0.445, 0.582,\n",
      "       0.391, 0.509, 0.219, 0.753, 0.326, 0.601, 0.601, 0.673, 1.137,\n",
      "       0.139, 0.329, 0.808, 0.635, 0.562, 0.235, 0.291, 0.471, 0.707,\n",
      "       0.33 , 0.477, 0.386, 0.525, 0.786, 0.639, 0.323, 0.423, 0.791])\n",
      " message: 'Optimization terminated successfully'\n",
      "    nfev: 192\n",
      "     nit: 3\n",
      "    njev: 3\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([0.025, 0.   , 0.01 , 0.01 , 0.   , 0.01 , 0.01 , 0.   , 0.01 ,\n",
      "       0.01 , 0.   , 0.   , 0.   , 0.01 , 0.095, 0.   , 0.042, 0.01 ,\n",
      "       0.01 , 0.01 , 0.01 , 0.01 , 0.01 , 0.1  , 0.   , 0.   , 0.024,\n",
      "       0.01 , 0.   , 0.   , 0.003, 0.01 , 0.   , 0.   , 0.   , 0.   ,\n",
      "       0.   , 0.01 , 0.1  , 0.01 , 0.084, 0.01 , 0.01 , 0.01 , 0.   ,\n",
      "       0.1  , 0.   , 0.01 , 0.01 , 0.01 , 0.01 , 0.01 , 0.01 , 0.   ,\n",
      "       0.055, 0.01 , 0.   , 0.   , 0.01 , 0.01 , 0.072, 0.   , 0.01 ])\n",
      "***** Version: 0.0.4 tol**-3 *****\n",
      "     fun: 0.19039506011999463\n",
      "     jac: array([0.374, 0.403, 1.111, 0.79 , 0.449, 0.514, 0.55 , 0.414, 0.46 ,\n",
      "       0.366, 0.434, 0.487, 0.505, 1.01 , 0.372, 0.461, 0.378, 1.068,\n",
      "       1.   , 0.816, 0.559, 0.546, 0.519, 0.261, 0.448, 0.607, 0.385,\n",
      "       1.027, 0.552, 0.406, 0.386, 0.955, 0.772, 0.634, 0.487, 0.624,\n",
      "       0.428, 0.551, 0.265, 0.822, 0.367, 0.658, 0.663, 0.721, 1.194,\n",
      "       0.187, 0.372, 0.872, 0.696, 0.619, 0.286, 0.344, 0.534, 0.753,\n",
      "       0.367, 0.523, 0.428, 0.576, 0.856, 0.716, 0.365, 0.458, 0.858])\n",
      " message: 'Optimization terminated successfully'\n",
      "    nfev: 192\n",
      "     nit: 3\n",
      "    njev: 3\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([0.019, 0.   , 0.01 , 0.01 , 0.   , 0.01 , 0.01 , 0.   , 0.01 ,\n",
      "       0.01 , 0.   , 0.   , 0.   , 0.01 , 0.087, 0.   , 0.041, 0.01 ,\n",
      "       0.01 , 0.01 , 0.01 , 0.01 , 0.01 , 0.1  , 0.   , 0.   , 0.023,\n",
      "       0.01 , 0.   , 0.   , 0.   , 0.01 , 0.   , 0.   , 0.   , 0.   ,\n",
      "       0.   , 0.01 , 0.1  , 0.01 , 0.095, 0.01 , 0.01 , 0.01 , 0.   ,\n",
      "       0.1  , 0.   , 0.01 , 0.01 , 0.01 , 0.01 , 0.01 , 0.01 , 0.   ,\n",
      "       0.057, 0.01 , 0.   , 0.   , 0.01 , 0.01 , 0.077, 0.   , 0.01 ])\n",
      "***** Version: 0.0.4 tol**-3 *****\n",
      "     fun: 0.21535872194648112\n",
      "     jac: array([0.411, 0.438, 1.197, 0.845, 0.485, 0.561, 0.598, 0.432, 0.507,\n",
      "       0.437, 0.474, 0.528, 0.556, 1.089, 0.411, 0.508, 0.417, 1.16 ,\n",
      "       1.075, 0.876, 0.614, 0.599, 0.603, 0.294, 0.492, 0.656, 0.427,\n",
      "       1.105, 0.588, 0.457, 0.435, 1.038, 0.819, 0.675, 0.529, 0.668,\n",
      "       0.468, 0.596, 0.311, 0.895, 0.403, 0.718, 0.727, 0.772, 1.253,\n",
      "       0.234, 0.415, 0.939, 0.759, 0.678, 0.338, 0.399, 0.597, 0.8  ,\n",
      "       0.407, 0.571, 0.472, 0.627, 0.927, 0.793, 0.406, 0.495, 0.926])\n",
      " message: 'Optimization terminated successfully'\n",
      "    nfev: 192\n",
      "     nit: 3\n",
      "    njev: 3\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([0.015, 0.   , 0.01 , 0.01 , 0.   , 0.01 , 0.01 , 0.   , 0.01 ,\n",
      "       0.01 , 0.   , 0.   , 0.   , 0.01 , 0.081, 0.   , 0.04 , 0.01 ,\n",
      "       0.01 , 0.01 , 0.01 , 0.01 , 0.01 , 0.1  , 0.   , 0.   , 0.023,\n",
      "       0.01 , 0.   , 0.   , 0.   , 0.01 , 0.   , 0.   , 0.   , 0.   ,\n",
      "       0.   , 0.01 , 0.1  , 0.01 , 0.1  , 0.01 , 0.01 , 0.01 , 0.   ,\n",
      "       0.1  , 0.   , 0.01 , 0.01 , 0.01 , 0.01 , 0.01 , 0.01 , 0.   ,\n",
      "       0.06 , 0.01 , 0.   , 0.   , 0.01 , 0.01 , 0.081, 0.   , 0.01 ])\n",
      "***** Version: 0.0.4 tol**-3 *****\n",
      "     fun: 0.2402342195734769\n",
      "     jac: array([0.45 , 0.473, 1.283, 0.902, 0.521, 0.61 , 0.648, 0.452, 0.555,\n",
      "       0.509, 0.515, 0.569, 0.607, 1.17 , 0.452, 0.555, 0.457, 1.255,\n",
      "       1.153, 0.937, 0.671, 0.654, 0.688, 0.328, 0.537, 0.706, 0.469,\n",
      "       1.184, 0.625, 0.508, 0.485, 1.122, 0.866, 0.717, 0.572, 0.711,\n",
      "       0.509, 0.642, 0.356, 0.969, 0.436, 0.779, 0.792, 0.824, 1.312,\n",
      "       0.28 , 0.458, 1.008, 0.824, 0.737, 0.392, 0.456, 0.66 , 0.848,\n",
      "       0.449, 0.621, 0.517, 0.678, 0.999, 0.87 , 0.448, 0.532, 0.995])\n",
      " message: 'Optimization terminated successfully'\n",
      "    nfev: 192\n",
      "     nit: 3\n",
      "    njev: 3\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([0.012, 0.   , 0.01 , 0.01 , 0.   , 0.01 , 0.01 , 0.   , 0.01 ,\n",
      "       0.01 , 0.   , 0.   , 0.   , 0.01 , 0.074, 0.   , 0.042, 0.01 ,\n",
      "       0.01 , 0.01 , 0.01 , 0.01 , 0.01 , 0.1  , 0.   , 0.   , 0.023,\n",
      "       0.01 , 0.   , 0.   , 0.   , 0.01 , 0.   , 0.   , 0.   , 0.   ,\n",
      "       0.   , 0.01 , 0.1  , 0.01 , 0.1  , 0.01 , 0.01 , 0.01 , 0.   ,\n",
      "       0.1  , 0.   , 0.01 , 0.01 , 0.01 , 0.01 , 0.01 , 0.01 , 0.   ,\n",
      "       0.064, 0.01 , 0.   , 0.   , 0.01 , 0.01 , 0.085, 0.   , 0.01 ])\n",
      "***** Version: 0.0.4 tol**-3 *****\n",
      "     fun: 0.26507111172207065\n",
      "     jac: array([0.491, 0.51 , 1.369, 0.96 , 0.558, 0.659, 0.699, 0.47 , 0.603,\n",
      "       0.579, 0.557, 0.61 , 0.66 , 1.252, 0.493, 0.604, 0.499, 1.351,\n",
      "       1.231, 1.   , 0.729, 0.709, 0.774, 0.362, 0.584, 0.759, 0.514,\n",
      "       1.266, 0.664, 0.554, 0.531, 1.206, 0.916, 0.761, 0.617, 0.755,\n",
      "       0.55 , 0.688, 0.402, 1.045, 0.466, 0.841, 0.859, 0.879, 1.373,\n",
      "       0.327, 0.502, 1.078, 0.891, 0.797, 0.445, 0.511, 0.721, 0.898,\n",
      "       0.494, 0.672, 0.563, 0.732, 1.072, 0.942, 0.485, 0.57 , 1.066])\n",
      " message: 'Optimization terminated successfully'\n",
      "    nfev: 192\n",
      "     nit: 3\n",
      "    njev: 3\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([0.009, 0.   , 0.01 , 0.01 , 0.   , 0.01 , 0.01 , 0.   , 0.01 ,\n",
      "       0.01 , 0.   , 0.   , 0.   , 0.01 , 0.069, 0.   , 0.044, 0.01 ,\n",
      "       0.01 , 0.01 , 0.01 , 0.01 , 0.01 , 0.1  , 0.   , 0.   , 0.022,\n",
      "       0.01 , 0.   , 0.   , 0.   , 0.01 , 0.   , 0.   , 0.   , 0.   ,\n",
      "       0.   , 0.01 , 0.1  , 0.01 , 0.1  , 0.01 , 0.01 , 0.01 , 0.   ,\n",
      "       0.1  , 0.   , 0.01 , 0.01 , 0.01 , 0.01 , 0.01 , 0.01 , 0.   ,\n",
      "       0.068, 0.01 , 0.   , 0.   , 0.01 , 0.01 , 0.087, 0.   , 0.01 ])\n",
      "***** Version: 0.0.4 tol**-3 *****\n",
      "     fun: 0.28970931028574765\n",
      "     jac: array([0.532, 0.546, 1.455, 1.018, 0.595, 0.708, 0.749, 0.486, 0.651,\n",
      "       0.65 , 0.599, 0.651, 0.713, 1.335, 0.535, 0.653, 0.542, 1.447,\n",
      "       1.31 , 1.063, 0.786, 0.765, 0.859, 0.397, 0.631, 0.812, 0.558,\n",
      "       1.348, 0.704, 0.6  , 0.576, 1.29 , 0.965, 0.806, 0.662, 0.798,\n",
      "       0.591, 0.735, 0.447, 1.121, 0.497, 0.904, 0.927, 0.934, 1.435,\n",
      "       0.374, 0.546, 1.149, 0.958, 0.858, 0.498, 0.567, 0.782, 0.948,\n",
      "       0.54 , 0.723, 0.609, 0.786, 1.145, 1.012, 0.519, 0.608, 1.138])\n",
      " message: 'Optimization terminated successfully'\n",
      "    nfev: 194\n",
      "     nit: 3\n",
      "    njev: 3\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([0.007, 0.   , 0.009, 0.01 , 0.   , 0.01 , 0.01 , 0.005, 0.01 ,\n",
      "       0.01 , 0.   , 0.   , 0.   , 0.01 , 0.064, 0.   , 0.046, 0.01 ,\n",
      "       0.01 , 0.01 , 0.01 , 0.01 , 0.01 , 0.1  , 0.   , 0.   , 0.019,\n",
      "       0.01 , 0.   , 0.   , 0.   , 0.01 , 0.   , 0.   , 0.   , 0.   ,\n",
      "       0.   , 0.01 , 0.1  , 0.01 , 0.1  , 0.01 , 0.01 , 0.01 , 0.001,\n",
      "       0.1  , 0.   , 0.01 , 0.01 , 0.01 , 0.01 , 0.01 , 0.01 , 0.   ,\n",
      "       0.072, 0.01 , 0.   , 0.   , 0.01 , 0.01 , 0.088, 0.   , 0.01 ])\n",
      "***** Version: 0.0.4 tol**-3 *****\n",
      "     fun: 0.33948261015919845\n",
      "     jac: array([0.566, 0.577, 1.516, 1.03 , 0.631, 0.728, 0.768, 0.516, 0.686,\n",
      "       0.777, 0.738, 0.692, 0.73 , 1.344, 0.565, 0.688, 0.49 , 1.505,\n",
      "       1.342, 1.062, 0.772, 0.758, 0.948, 0.382, 0.598, 0.806, 0.506,\n",
      "       1.38 , 0.7  , 0.688, 0.663, 1.373, 0.986, 0.772, 0.692, 0.843,\n",
      "       0.634, 0.757, 0.623, 1.162, 0.555, 0.918, 0.955, 0.923, 1.468,\n",
      "       0.544, 0.96 , 1.175, 0.988, 0.931, 0.546, 0.62 , 0.92 , 0.886,\n",
      "       0.475, 0.757, 0.612, 0.797, 1.221, 1.132, 0.588, 0.619, 1.15 ])\n",
      " message: 'Optimization terminated successfully'\n",
      "    nfev: 139\n",
      "     nit: 2\n",
      "    njev: 2\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([0.011, 0.   , 0.01 , 0.01 , 0.   , 0.01 , 0.01 , 0.   , 0.01 ,\n",
      "       0.01 , 0.   , 0.   , 0.   , 0.01 , 0.059, 0.   , 0.006, 0.01 ,\n",
      "       0.01 , 0.01 , 0.01 , 0.01 , 0.01 , 0.1  , 0.   , 0.   , 0.   ,\n",
      "       0.01 , 0.   , 0.   , 0.005, 0.01 , 0.   , 0.   , 0.   , 0.   ,\n",
      "       0.   , 0.01 , 0.1  , 0.01 , 0.1  , 0.01 , 0.01 , 0.01 , 0.   ,\n",
      "       0.1  , 0.1  , 0.01 , 0.01 , 0.01 , 0.01 , 0.01 , 0.01 , 0.   ,\n",
      "       0.02 , 0.01 , 0.   , 0.   , 0.01 , 0.01 , 0.1  , 0.   , 0.01 ])\n",
      "***** Version: 0.0.4 tol**-3 *****\n",
      "     fun: 0.33872619904087914\n",
      "     jac: array([0.612, 0.616, 1.605, 1.122, 0.66 , 0.798, 0.841, 0.526, 0.742,\n",
      "       0.782, 0.685, 0.731, 0.808, 1.473, 0.609, 0.734, 0.61 , 1.576,\n",
      "       1.454, 1.168, 0.891, 0.865, 1.018, 0.458, 0.704, 0.897, 0.615,\n",
      "       1.492, 0.768, 0.704, 0.678, 1.436, 1.067, 0.881, 0.737, 0.882,\n",
      "       0.668, 0.817, 0.539, 1.252, 0.571, 1.012, 1.044, 1.027, 1.602,\n",
      "       0.458, 0.638, 1.271, 1.072, 0.962, 0.593, 0.665, 0.916, 1.036,\n",
      "       0.604, 0.812, 0.687, 0.884, 1.274, 1.179, 0.623, 0.67 , 1.258])\n",
      " message: 'Optimization terminated successfully'\n",
      "    nfev: 332\n",
      "     nit: 5\n",
      "    njev: 5\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([0.012, 0.008, 0.01 , 0.01 , 0.   , 0.01 , 0.01 , 0.   , 0.01 ,\n",
      "       0.01 , 0.   , 0.   , 0.   , 0.01 , 0.061, 0.   , 0.05 , 0.   ,\n",
      "       0.01 , 0.01 , 0.01 , 0.01 , 0.01 , 0.1  , 0.   , 0.   , 0.004,\n",
      "       0.01 , 0.   , 0.   , 0.   , 0.01 , 0.   , 0.   , 0.   , 0.   ,\n",
      "       0.   , 0.01 , 0.1  , 0.01 , 0.1  , 0.01 , 0.01 , 0.01 , 0.01 ,\n",
      "       0.1  , 0.   , 0.01 , 0.01 , 0.01 , 0.01 , 0.01 , 0.01 , 0.   ,\n",
      "       0.065, 0.01 , 0.   , 0.   , 0.01 , 0.01 , 0.1  , 0.   , 0.01 ])\n",
      "***** Version: 0.0.4 tol**-3 *****\n",
      "     fun: 0.3630580660206247\n",
      "     jac: array([0.643, 0.648, 1.701, 1.184, 0.7  , 0.849, 0.894, 0.551, 0.79 ,\n",
      "       0.857, 0.718, 0.774, 0.865, 1.567, 0.641, 0.79 , 0.66 , 1.694,\n",
      "       1.534, 1.24 , 0.952, 0.924, 1.107, 0.495, 0.76 , 0.957, 0.677,\n",
      "       1.576, 0.812, 0.751, 0.724, 1.527, 1.114, 0.934, 0.787, 0.926,\n",
      "       0.709, 0.867, 0.578, 1.331, 0.599, 1.084, 1.12 , 1.083, 1.642,\n",
      "       0.506, 0.671, 1.346, 1.143, 1.027, 0.652, 0.728, 0.975, 1.089,\n",
      "       0.667, 0.868, 0.736, 0.94 , 1.35 , 1.247, 0.659, 0.714, 1.339])\n",
      " message: 'Optimization terminated successfully'\n",
      "    nfev: 194\n",
      "     nit: 3\n",
      "    njev: 3\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([0.002, 0.   , 0.009, 0.01 , 0.   , 0.01 , 0.01 , 0.008, 0.01 ,\n",
      "       0.01 , 0.   , 0.   , 0.   , 0.01 , 0.047, 0.   , 0.051, 0.005,\n",
      "       0.01 , 0.01 , 0.01 , 0.01 , 0.01 , 0.1  , 0.   , 0.   , 0.015,\n",
      "       0.01 , 0.   , 0.   , 0.   , 0.01 , 0.   , 0.   , 0.   , 0.   ,\n",
      "       0.   , 0.01 , 0.1  , 0.01 , 0.1  , 0.01 , 0.01 , 0.01 , 0.006,\n",
      "       0.1  , 0.   , 0.01 , 0.01 , 0.01 , 0.01 , 0.01 , 0.01 , 0.   ,\n",
      "       0.079, 0.01 , 0.   , 0.   , 0.01 , 0.01 , 0.097, 0.   , 0.01 ])\n",
      "***** Version: 0.0.4 tol**-3 *****\n",
      "     fun: 0.4198212818449682\n",
      "     jac: array([0.659, 0.653, 1.694, 1.177, 0.726, 0.85 , 0.898, 0.937, 0.803,\n",
      "       0.94 , 0.849, 0.801, 0.853, 1.532, 0.679, 0.832, 0.586, 1.685,\n",
      "       1.536, 1.208, 0.92 , 0.898, 1.171, 0.467, 0.706, 0.931, 0.61 ,\n",
      "       1.576, 0.794, 0.846, 0.815, 1.545, 1.11 , 0.889, 0.807, 0.953,\n",
      "       0.743, 0.876, 0.776, 1.334, 0.646, 1.081, 1.138, 1.042, 1.681,\n",
      "       0.699, 1.163, 1.353, 1.158, 1.105, 0.693, 0.775, 1.132, 0.988,\n",
      "       0.567, 0.909, 0.723, 0.938, 1.421, 1.376, 0.725, 0.713, 1.324])\n",
      " message: 'Optimization terminated successfully'\n",
      "    nfev: 139\n",
      "     nit: 2\n",
      "    njev: 2\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([0.003, 0.   , 0.01 , 0.01 , 0.   , 0.01 , 0.01 , 0.023, 0.01 ,\n",
      "       0.01 , 0.   , 0.   , 0.   , 0.01 , 0.05 , 0.   , 0.003, 0.   ,\n",
      "       0.01 , 0.01 , 0.01 , 0.01 , 0.01 , 0.1  , 0.   , 0.   , 0.   ,\n",
      "       0.01 , 0.   , 0.   , 0.   , 0.01 , 0.   , 0.   , 0.   , 0.   ,\n",
      "       0.   , 0.01 , 0.1  , 0.01 , 0.1  , 0.01 , 0.01 , 0.01 , 0.01 ,\n",
      "       0.1  , 0.1  , 0.01 , 0.01 , 0.01 , 0.01 , 0.01 , 0.01 , 0.   ,\n",
      "       0.021, 0.01 , 0.   , 0.   , 0.01 , 0.01 , 0.1  , 0.   , 0.01 ])\n",
      "***** Version: 0.0.4 tol**-3 *****\n",
      "     fun: 0.4108674424305966\n",
      "     jac: array([0.724, 0.719, 1.835, 1.293, 0.769, 0.937, 0.987, 0.757, 0.875,\n",
      "       0.969, 0.795, 0.848, 0.956, 1.709, 0.72 , 0.885, 0.738, 1.839,\n",
      "       1.678, 1.349, 1.058, 1.025, 1.262, 0.558, 0.84 , 1.051, 0.748,\n",
      "       1.723, 0.884, 0.846, 0.817, 1.659, 1.207, 1.021, 0.869, 1.003,\n",
      "       0.785, 0.951, 0.67 , 1.46 , 0.66 , 1.201, 1.249, 1.179, 1.787,\n",
      "       0.595, 0.767, 1.475, 1.266, 1.145, 0.75 , 0.832, 1.106, 1.178,\n",
      "       0.745, 0.971, 0.816, 1.044, 1.486, 1.397, 0.732, 0.782, 1.467])\n",
      " message: 'Optimization terminated successfully'\n",
      "    nfev: 257\n",
      "     nit: 4\n",
      "    njev: 4\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([0.004, 0.008, 0.01 , 0.01 , 0.   , 0.01 , 0.01 , 0.011, 0.01 ,\n",
      "       0.01 , 0.   , 0.   , 0.   , 0.01 , 0.044, 0.   , 0.054, 0.   ,\n",
      "       0.01 , 0.01 , 0.01 , 0.01 , 0.01 , 0.1  , 0.   , 0.   , 0.007,\n",
      "       0.01 , 0.   , 0.   , 0.   , 0.01 , 0.   , 0.   , 0.   , 0.   ,\n",
      "       0.   , 0.01 , 0.1  , 0.01 , 0.1  , 0.01 , 0.01 , 0.01 , 0.01 ,\n",
      "       0.1  , 0.   , 0.01 , 0.01 , 0.01 , 0.01 , 0.01 , 0.01 , 0.   ,\n",
      "       0.078, 0.01 , 0.   , 0.   , 0.01 , 0.01 , 0.094, 0.   , 0.01 ])\n",
      "***** Version: 0.0.4 tol**-3 *****\n",
      "     fun: 0.43460728313400604\n",
      "     jac: array([0.742, 0.777, 1.914, 1.347, 0.798, 0.981, 1.032, 0.883, 0.916,\n",
      "       1.029, 0.831, 0.884, 0.995, 1.773, 0.747, 0.929, 0.776, 1.931,\n",
      "       1.758, 1.396, 1.118, 1.078, 1.345, 0.586, 0.88 , 1.087, 0.792,\n",
      "       1.798, 0.91 , 0.918, 0.885, 1.739, 1.248, 1.052, 0.898, 1.044,\n",
      "       0.819, 0.985, 0.711, 1.521, 0.697, 1.252, 1.305, 1.223, 1.837,\n",
      "       0.638, 0.809, 1.539, 1.323, 1.191, 0.804, 0.89 , 1.186, 1.215,\n",
      "       0.731, 1.019, 0.858, 1.08 , 1.557, 1.498, 0.804, 0.81 , 1.53 ])\n",
      " message: 'Optimization terminated successfully'\n",
      "    nfev: 257\n",
      "     nit: 4\n",
      "    njev: 4\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([0.011, 0.018, 0.01 , 0.01 , 0.   , 0.01 , 0.01 , 0.011, 0.01 ,\n",
      "       0.01 , 0.   , 0.   , 0.   , 0.01 , 0.053, 0.   , 0.047, 0.   ,\n",
      "       0.01 , 0.01 , 0.01 , 0.01 , 0.01 , 0.1  , 0.   , 0.   , 0.   ,\n",
      "       0.01 , 0.   , 0.   , 0.   , 0.01 , 0.   , 0.   , 0.   , 0.   ,\n",
      "       0.   , 0.01 , 0.1  , 0.01 , 0.1  , 0.01 , 0.01 , 0.01 , 0.01 ,\n",
      "       0.1  , 0.   , 0.01 , 0.01 , 0.01 , 0.01 , 0.01 , 0.01 , 0.   ,\n",
      "       0.067, 0.01 , 0.   , 0.   , 0.01 , 0.01 , 0.094, 0.   , 0.01 ])\n",
      "***** Version: 0.0.4 tol**-3 *****\n",
      "     fun: 0.4587743488908774\n",
      "     jac: array([0.782, 0.83 , 2.006, 1.408, 0.835, 1.03 , 1.082, 0.868, 0.966,\n",
      "       1.103, 0.879, 0.927, 1.05 , 1.855, 0.788, 0.977, 0.82 , 2.034,\n",
      "       1.843, 1.459, 1.179, 1.135, 1.432, 0.622, 0.928, 1.139, 0.839,\n",
      "       1.886, 0.95 , 0.964, 0.931, 1.83 , 1.302, 1.094, 0.941, 1.091,\n",
      "       0.86 , 1.028, 0.759, 1.601, 0.73 , 1.313, 1.371, 1.281, 1.905,\n",
      "       0.684, 0.856, 1.612, 1.391, 1.248, 0.856, 0.945, 1.252, 1.266,\n",
      "       0.76 , 1.069, 0.908, 1.132, 1.632, 1.568, 0.84 , 0.848, 1.601])\n",
      " message: 'Optimization terminated successfully'\n",
      "    nfev: 257\n",
      "     nit: 4\n",
      "    njev: 4\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([0.01 , 0.017, 0.01 , 0.01 , 0.   , 0.01 , 0.01 , 0.016, 0.01 ,\n",
      "       0.01 , 0.   , 0.   , 0.   , 0.01 , 0.05 , 0.   , 0.048, 0.   ,\n",
      "       0.01 , 0.01 , 0.01 , 0.01 , 0.01 , 0.1  , 0.   , 0.   , 0.   ,\n",
      "       0.01 , 0.   , 0.   , 0.   , 0.01 , 0.   , 0.   , 0.   , 0.   ,\n",
      "       0.   , 0.01 , 0.1  , 0.01 , 0.1  , 0.01 , 0.01 , 0.01 , 0.01 ,\n",
      "       0.1  , 0.   , 0.01 , 0.01 , 0.01 , 0.01 , 0.01 , 0.01 , 0.   ,\n",
      "       0.071, 0.01 , 0.   , 0.   , 0.01 , 0.01 , 0.088, 0.   , 0.01 ])\n",
      "***** Version: 0.0.4 tol**-3 *****\n",
      "     fun: 0.5644509136018752\n",
      "     jac: array([0.773, 0.722, 1.898, 1.357, 0.848, 1.013, 1.077, 2.088, 0.94 ,\n",
      "       1.114, 0.959, 0.923, 0.971, 1.763, 0.783, 1.031, 0.704, 1.943,\n",
      "       1.766, 1.369, 1.114, 1.066, 1.464, 0.559, 0.832, 1.065, 0.744,\n",
      "       1.793, 0.887, 1.138, 1.088, 1.742, 1.21 , 1.024, 0.936, 1.077,\n",
      "       0.879, 1.032, 0.963, 1.508, 0.767, 1.282, 1.369, 1.159, 1.828,\n",
      "       0.908, 1.414, 1.571, 1.37 , 1.34 , 0.919, 1.016, 1.439, 1.076,\n",
      "       0.626, 1.125, 0.848, 1.083, 1.677, 1.782, 0.986, 0.817, 1.556])\n",
      " message: 'Optimization terminated successfully'\n",
      "    nfev: 139\n",
      "     nit: 2\n",
      "    njev: 2\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([0.   , 0.   , 0.01 , 0.01 , 0.   , 0.01 , 0.01 , 0.072, 0.01 ,\n",
      "       0.01 , 0.   , 0.   , 0.   , 0.01 , 0.021, 0.   , 0.   , 0.   ,\n",
      "       0.01 , 0.01 , 0.01 , 0.01 , 0.01 , 0.1  , 0.   , 0.   , 0.   ,\n",
      "       0.01 , 0.   , 0.   , 0.   , 0.01 , 0.   , 0.   , 0.   , 0.   ,\n",
      "       0.   , 0.01 , 0.1  , 0.01 , 0.1  , 0.01 , 0.01 , 0.01 , 0.01 ,\n",
      "       0.1  , 0.1  , 0.01 , 0.01 , 0.01 , 0.01 , 0.01 , 0.01 , 0.   ,\n",
      "       0.006, 0.01 , 0.   , 0.   , 0.01 , 0.01 , 0.1  , 0.   , 0.01 ])\n",
      "***** Version: 0.0.4 tol**-3 *****\n",
      "     fun: 0.5095298466431346\n",
      "     jac: array([0.919, 0.869, 2.196, 1.526, 0.923, 1.14 , 1.192, 0.631, 1.078,\n",
      "       1.27 , 0.983, 1.022, 1.177, 2.047, 0.898, 1.072, 0.88 , 2.223,\n",
      "       1.988, 1.606, 1.276, 1.241, 1.603, 0.692, 1.013, 1.266, 0.896,\n",
      "       2.05 , 1.045, 1.018, 0.989, 2.012, 1.415, 1.197, 1.056, 1.182,\n",
      "       0.96 , 1.149, 0.859, 1.774, 0.787, 1.452, 1.518, 1.402, 2.06 ,\n",
      "       0.786, 0.958, 1.753, 1.533, 1.397, 0.963, 1.052, 1.341, 1.382,\n",
      "       0.974, 1.171, 0.998, 1.271, 1.771, 1.673, 0.861, 0.938, 1.749])\n",
      " message: 'Optimization terminated successfully'\n",
      "    nfev: 203\n",
      "     nit: 3\n",
      "    njev: 3\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([0.01 , 0.005, 0.01 , 0.01 , 0.   , 0.01 , 0.01 , 0.   , 0.01 ,\n",
      "       0.01 , 0.   , 0.   , 0.   , 0.01 , 0.044, 0.   , 0.043, 0.   ,\n",
      "       0.01 , 0.01 , 0.01 , 0.01 , 0.01 , 0.1  , 0.   , 0.   , 0.   ,\n",
      "       0.01 , 0.   , 0.   , 0.   , 0.01 , 0.   , 0.   , 0.   , 0.   ,\n",
      "       0.   , 0.01 , 0.1  , 0.01 , 0.1  , 0.01 , 0.01 , 0.01 , 0.01 ,\n",
      "       0.1  , 0.   , 0.01 , 0.01 , 0.01 , 0.01 , 0.01 , 0.01 , 0.   ,\n",
      "       0.1  , 0.01 , 0.   , 0.   , 0.01 , 0.01 , 0.098, 0.   , 0.01 ])\n",
      "***** Version: 0.0.4 tol**-3 *****\n",
      "     fun: 0.5304437701271433\n",
      "     jac: array([0.926, 0.912, 2.246, 1.572, 0.946, 1.179, 1.235, 1.005, 1.109,\n",
      "       1.302, 1.002, 1.047, 1.199, 2.094, 0.919, 1.121, 0.918, 2.294,\n",
      "       2.057, 1.638, 1.336, 1.289, 1.677, 0.716, 1.048, 1.289, 0.942,\n",
      "       2.108, 1.061, 1.111, 1.074, 2.062, 1.435, 1.222, 1.076, 1.213,\n",
      "       0.988, 1.177, 0.897, 1.812, 0.82 , 1.495, 1.568, 1.431, 2.087,\n",
      "       0.828, 0.995, 1.81 , 1.583, 1.441, 1.02 , 1.115, 1.431, 1.402,\n",
      "       0.928, 1.224, 1.033, 1.293, 1.837, 1.796, 0.953, 0.958, 1.805])\n",
      " message: 'Optimization terminated successfully'\n",
      "    nfev: 320\n",
      "     nit: 5\n",
      "    njev: 5\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([0.006, 0.02 , 0.01 , 0.01 , 0.   , 0.01 , 0.01 , 0.016, 0.01 ,\n",
      "       0.01 , 0.   , 0.   , 0.   , 0.01 , 0.039, 0.   , 0.046, 0.   ,\n",
      "       0.01 , 0.01 , 0.01 , 0.01 , 0.01 , 0.1  , 0.   , 0.   , 0.   ,\n",
      "       0.01 , 0.   , 0.   , 0.   , 0.01 , 0.   , 0.   , 0.   , 0.   ,\n",
      "       0.   , 0.01 , 0.1  , 0.01 , 0.1  , 0.01 , 0.01 , 0.01 , 0.01 ,\n",
      "       0.1  , 0.   , 0.01 , 0.01 , 0.01 , 0.01 , 0.01 , 0.01 , 0.   ,\n",
      "       0.072, 0.01 , 0.   , 0.   , 0.01 , 0.01 , 0.1  , 0.   , 0.01 ])\n",
      "***** Version: 0.0.4 tol**-3 *****\n",
      "     fun: 0.5543126855156794\n",
      "     jac: array([0.968, 0.949, 2.328, 1.629, 0.982, 1.228, 1.285, 1.048, 1.156,\n",
      "       1.369, 1.044, 1.087, 1.249, 2.173, 0.959, 1.169, 0.957, 2.386,\n",
      "       2.134, 1.698, 1.393, 1.343, 1.76 , 0.749, 1.092, 1.339, 0.984,\n",
      "       2.187, 1.098, 1.161, 1.123, 2.142, 1.482, 1.264, 1.119, 1.256,\n",
      "       1.029, 1.223, 0.943, 1.885, 0.851, 1.555, 1.633, 1.483, 2.149,\n",
      "       0.874, 1.041, 1.878, 1.648, 1.5  , 1.074, 1.171, 1.495, 1.449,\n",
      "       0.968, 1.275, 1.078, 1.345, 1.908, 1.872, 0.993, 0.994, 1.873])\n",
      " message: 'Optimization terminated successfully'\n",
      "    nfev: 320\n",
      "     nit: 5\n",
      "    njev: 5\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([0.006, 0.02 , 0.01 , 0.01 , 0.   , 0.01 , 0.01 , 0.017, 0.01 ,\n",
      "       0.01 , 0.   , 0.   , 0.   , 0.01 , 0.038, 0.   , 0.046, 0.   ,\n",
      "       0.01 , 0.01 , 0.01 , 0.01 , 0.01 , 0.1  , 0.   , 0.   , 0.   ,\n",
      "       0.01 , 0.   , 0.   , 0.   , 0.01 , 0.   , 0.   , 0.   , 0.   ,\n",
      "       0.   , 0.01 , 0.1  , 0.01 , 0.1  , 0.01 , 0.01 , 0.01 , 0.01 ,\n",
      "       0.1  , 0.   , 0.01 , 0.01 , 0.01 , 0.01 , 0.01 , 0.01 , 0.   ,\n",
      "       0.072, 0.01 , 0.   , 0.   , 0.01 , 0.01 , 0.1  , 0.   , 0.01 ])\n",
      "***** Version: 0.0.4 tol**-3 *****\n",
      "     fun: 0.5781226330356956\n",
      "     jac: array([1.014, 0.986, 2.414, 1.686, 1.02 , 1.278, 1.336, 1.074, 1.206,\n",
      "       1.439, 1.09 , 1.128, 1.301, 2.253, 1.004, 1.217, 0.997, 2.48 ,\n",
      "       2.213, 1.758, 1.451, 1.398, 1.845, 0.783, 1.137, 1.388, 1.026,\n",
      "       2.268, 1.135, 1.212, 1.172, 2.226, 1.53 , 1.304, 1.162, 1.299,\n",
      "       1.071, 1.27 , 0.991, 1.96 , 0.882, 1.613, 1.696, 1.537, 2.21 ,\n",
      "       0.921, 1.087, 1.948, 1.715, 1.56 , 1.128, 1.227, 1.558, 1.495,\n",
      "       1.002, 1.325, 1.125, 1.396, 1.981, 1.947, 1.031, 1.029, 1.942])\n",
      " message: 'Optimization terminated successfully'\n",
      "    nfev: 331\n",
      "     nit: 5\n",
      "    njev: 5\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([0.009, 0.021, 0.01 , 0.01 , 0.   , 0.01 , 0.01 , 0.017, 0.01 ,\n",
      "       0.01 , 0.   , 0.   , 0.   , 0.01 , 0.038, 0.   , 0.046, 0.   ,\n",
      "       0.01 , 0.01 , 0.01 , 0.01 , 0.01 , 0.1  , 0.   , 0.   , 0.   ,\n",
      "       0.01 , 0.   , 0.   , 0.   , 0.01 , 0.   , 0.   , 0.   , 0.   ,\n",
      "       0.   , 0.01 , 0.1  , 0.01 , 0.1  , 0.01 , 0.01 , 0.01 , 0.01 ,\n",
      "       0.1  , 0.   , 0.01 , 0.01 , 0.01 , 0.01 , 0.01 , 0.01 , 0.   ,\n",
      "       0.07 , 0.01 , 0.   , 0.   , 0.01 , 0.01 , 0.1  , 0.   , 0.01 ])\n",
      "***** Version: 0.0.4 tol**-3 *****\n",
      "     fun: 0.7530907063065293\n",
      "     jac: array([0.936, 0.831, 2.194, 1.595, 1.009, 1.226, 1.306, 3.187, 1.127,\n",
      "       1.369, 1.126, 1.089, 1.145, 2.076, 0.928, 1.278, 0.862, 2.298,\n",
      "       2.074, 1.594, 1.36 , 1.288, 1.845, 0.684, 1.002, 1.252, 0.917,\n",
      "       2.093, 1.022, 1.471, 1.403, 2.03 , 1.364, 1.202, 1.112, 1.248,\n",
      "       1.059, 1.235, 1.207, 1.761, 0.922, 1.544, 1.667, 1.332, 2.045,\n",
      "       1.172, 1.736, 1.858, 1.649, 1.638, 1.195, 1.31 , 1.81 , 1.212,\n",
      "       0.732, 1.39 , 1.019, 1.285, 2.006, 2.252, 1.276, 0.96 , 1.858])\n",
      " message: 'Optimization terminated successfully'\n",
      "    nfev: 128\n",
      "     nit: 2\n",
      "    njev: 2\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([0.  , 0.  , 0.01, 0.01, 0.  , 0.01, 0.01, 0.1 , 0.01, 0.01, 0.  ,\n",
      "       0.  , 0.  , 0.01, 0.  , 0.  , 0.  , 0.  , 0.01, 0.01, 0.01, 0.01,\n",
      "       0.01, 0.1 , 0.  , 0.  , 0.  , 0.01, 0.  , 0.  , 0.  , 0.01, 0.  ,\n",
      "       0.  , 0.  , 0.  , 0.  , 0.01, 0.1 , 0.01, 0.1 , 0.01, 0.01, 0.01,\n",
      "       0.01, 0.1 , 0.1 , 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.  , 0.  ,\n",
      "       0.01, 0.  , 0.  , 0.01, 0.01, 0.1 , 0.  , 0.01])\n",
      "***** Version: 0.0.4 tol**-3 *****\n",
      "     fun: 0.7817493656513509\n",
      "     jac: array([0.973, 0.86 , 2.266, 1.646, 1.044, 1.271, 1.353, 3.296, 1.171,\n",
      "       1.433, 1.169, 1.126, 1.189, 2.147, 0.967, 1.326, 0.895, 2.38 ,\n",
      "       2.144, 1.647, 1.412, 1.337, 1.926, 0.713, 1.041, 1.296, 0.953,\n",
      "       2.164, 1.054, 1.528, 1.458, 2.101, 1.404, 1.238, 1.152, 1.288,\n",
      "       1.099, 1.278, 1.26 , 1.825, 0.953, 1.6  , 1.729, 1.376, 2.099,\n",
      "       1.226, 1.804, 1.922, 1.71 , 1.698, 1.249, 1.367, 1.879, 1.247,\n",
      "       0.76 , 1.442, 1.059, 1.331, 2.077, 2.335, 1.321, 0.993, 1.922])\n",
      " message: 'Optimization terminated successfully'\n",
      "    nfev: 128\n",
      "     nit: 2\n",
      "    njev: 2\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([0.  , 0.  , 0.01, 0.01, 0.  , 0.01, 0.01, 0.1 , 0.01, 0.01, 0.  ,\n",
      "       0.  , 0.  , 0.01, 0.  , 0.  , 0.  , 0.  , 0.01, 0.01, 0.01, 0.01,\n",
      "       0.01, 0.1 , 0.  , 0.  , 0.  , 0.01, 0.  , 0.  , 0.  , 0.01, 0.  ,\n",
      "       0.  , 0.  , 0.  , 0.  , 0.01, 0.1 , 0.01, 0.1 , 0.01, 0.01, 0.01,\n",
      "       0.01, 0.1 , 0.1 , 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.  , 0.  ,\n",
      "       0.01, 0.  , 0.  , 0.01, 0.01, 0.1 , 0.  , 0.01])\n",
      "***** Version: 0.0.4 tol**-3 *****\n",
      "     fun: 0.6499145521172116\n",
      "     jac: array([1.129, 1.158, 2.694, 1.886, 1.139, 1.432, 1.495, 1.017, 1.361,\n",
      "       1.657, 1.212, 1.264, 1.48 , 2.536, 1.141, 1.38 , 1.16 , 2.819,\n",
      "       2.502, 1.984, 1.656, 1.597, 2.111, 0.908, 1.303, 1.576, 1.187,\n",
      "       2.569, 1.276, 1.322, 1.286, 2.506, 1.717, 1.471, 1.311, 1.445,\n",
      "       1.202, 1.417, 1.07 , 2.227, 0.966, 1.833, 1.929, 1.747, 2.456,\n",
      "       1.016, 1.192, 2.202, 1.943, 1.737, 1.283, 1.394, 1.742, 1.677,\n",
      "       1.158, 1.494, 1.286, 1.583, 2.219, 2.129, 1.107, 1.158, 2.187])\n",
      " message: 'Optimization terminated successfully'\n",
      "    nfev: 258\n",
      "     nit: 4\n",
      "    njev: 4\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([0.016, 0.025, 0.01 , 0.01 , 0.   , 0.01 , 0.01 , 0.017, 0.01 ,\n",
      "       0.01 , 0.   , 0.   , 0.   , 0.01 , 0.039, 0.   , 0.046, 0.   ,\n",
      "       0.01 , 0.01 , 0.01 , 0.01 , 0.01 , 0.1  , 0.   , 0.   , 0.   ,\n",
      "       0.01 , 0.   , 0.   , 0.   , 0.01 , 0.   , 0.   , 0.   , 0.   ,\n",
      "       0.   , 0.01 , 0.1  , 0.01 , 0.1  , 0.01 , 0.01 , 0.01 , 0.01 ,\n",
      "       0.1  , 0.   , 0.01 , 0.01 , 0.01 , 0.01 , 0.01 , 0.01 , 0.   ,\n",
      "       0.057, 0.01 , 0.   , 0.   , 0.01 , 0.01 , 0.1  , 0.   , 0.01 ])\n",
      "***** Version: 0.0.4 tol**-3 *****\n",
      "     fun: 0.8390666845333237\n",
      "     jac: array([1.048, 0.919, 2.411, 1.749, 1.113, 1.362, 1.448, 3.513, 1.258,\n",
      "       1.56 , 1.253, 1.201, 1.276, 2.287, 1.045, 1.424, 0.962, 2.544,\n",
      "       2.284, 1.752, 1.516, 1.434, 2.088, 0.771, 1.117, 1.383, 1.027,\n",
      "       2.305, 1.118, 1.642, 1.567, 2.243, 1.484, 1.311, 1.231, 1.367,\n",
      "       1.178, 1.364, 1.366, 1.953, 1.016, 1.71 , 1.852, 1.464, 2.207,\n",
      "       1.335, 1.94 , 2.049, 1.832, 1.819, 1.357, 1.48 , 2.019, 1.318,\n",
      "       0.817, 1.544, 1.14 , 1.423, 2.217, 2.501, 1.413, 1.058, 2.048])\n",
      " message: 'Optimization terminated successfully'\n",
      "    nfev: 139\n",
      "     nit: 2\n",
      "    njev: 2\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([0.  , 0.  , 0.01, 0.01, 0.  , 0.01, 0.01, 0.1 , 0.01, 0.01, 0.  ,\n",
      "       0.  , 0.  , 0.01, 0.  , 0.  , 0.  , 0.  , 0.01, 0.01, 0.01, 0.01,\n",
      "       0.01, 0.1 , 0.  , 0.  , 0.  , 0.01, 0.  , 0.  , 0.  , 0.01, 0.  ,\n",
      "       0.  , 0.  , 0.  , 0.  , 0.01, 0.1 , 0.01, 0.1 , 0.01, 0.01, 0.01,\n",
      "       0.01, 0.1 , 0.1 , 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.  , 0.  ,\n",
      "       0.01, 0.  , 0.  , 0.01, 0.01, 0.1 , 0.  , 0.01])\n",
      "***** Version: 0.0.4 tol**-3 *****\n",
      "     fun: 0.8677253438866959\n",
      "     jac: array([1.085, 0.948, 2.484, 1.8  , 1.148, 1.407, 1.495, 3.621, 1.301,\n",
      "       1.623, 1.296, 1.238, 1.32 , 2.357, 1.084, 1.472, 0.995, 2.627,\n",
      "       2.354, 1.804, 1.568, 1.483, 2.169, 0.8  , 1.155, 1.426, 1.064,\n",
      "       2.376, 1.15 , 1.699, 1.622, 2.314, 1.523, 1.348, 1.271, 1.407,\n",
      "       1.217, 1.408, 1.419, 2.017, 1.047, 1.766, 1.914, 1.508, 2.261,\n",
      "       1.389, 2.007, 2.112, 1.893, 1.879, 1.41 , 1.537, 2.089, 1.353,\n",
      "       0.845, 1.595, 1.18 , 1.469, 2.288, 2.584, 1.458, 1.091, 2.111])\n",
      " message: 'Optimization terminated successfully'\n",
      "    nfev: 139\n",
      "     nit: 2\n",
      "    njev: 2\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([0.  , 0.  , 0.01, 0.01, 0.  , 0.01, 0.01, 0.1 , 0.01, 0.01, 0.  ,\n",
      "       0.  , 0.  , 0.01, 0.  , 0.  , 0.  , 0.  , 0.01, 0.01, 0.01, 0.01,\n",
      "       0.01, 0.1 , 0.  , 0.  , 0.  , 0.01, 0.  , 0.  , 0.  , 0.01, 0.  ,\n",
      "       0.  , 0.  , 0.  , 0.  , 0.01, 0.1 , 0.01, 0.1 , 0.01, 0.01, 0.01,\n",
      "       0.01, 0.1 , 0.1 , 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.  , 0.  ,\n",
      "       0.01, 0.  , 0.  , 0.01, 0.01, 0.1 , 0.  , 0.01])\n",
      "***** Version: 0.0.4 tol**-3 *****\n",
      "     fun: 0.7206453407036636\n",
      "     jac: array([1.266, 1.254, 2.947, 2.05 , 1.256, 1.584, 1.649, 1.181, 1.506,\n",
      "       1.861, 1.351, 1.379, 1.625, 2.764, 1.264, 1.519, 1.264, 3.088,\n",
      "       2.73 , 2.154, 1.829, 1.759, 2.366, 1.004, 1.427, 1.711, 1.306,\n",
      "       2.8  , 1.375, 1.496, 1.453, 2.749, 1.844, 1.575, 1.43 , 1.575,\n",
      "       1.329, 1.556, 1.224, 2.439, 1.063, 1.995, 2.107, 1.898, 2.619,\n",
      "       1.165, 1.337, 2.402, 2.132, 1.912, 1.449, 1.567, 1.94 , 1.8  ,\n",
      "       1.229, 1.641, 1.421, 1.721, 2.431, 2.377, 1.246, 1.257, 2.386])\n",
      " message: 'Optimization terminated successfully'\n",
      "    nfev: 324\n",
      "     nit: 5\n",
      "    njev: 5\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([0.005, 0.033, 0.01 , 0.01 , 0.005, 0.01 , 0.01 , 0.018, 0.01 ,\n",
      "       0.01 , 0.   , 0.   , 0.   , 0.01 , 0.034, 0.   , 0.049, 0.   ,\n",
      "       0.01 , 0.01 , 0.01 , 0.01 , 0.01 , 0.1  , 0.   , 0.   , 0.   ,\n",
      "       0.01 , 0.   , 0.   , 0.   , 0.01 , 0.   , 0.   , 0.   , 0.   ,\n",
      "       0.   , 0.01 , 0.093, 0.01 , 0.1  , 0.01 , 0.01 , 0.01 , 0.01 ,\n",
      "       0.1  , 0.   , 0.01 , 0.01 , 0.01 , 0.01 , 0.01 , 0.01 , 0.   ,\n",
      "       0.069, 0.01 , 0.   , 0.   , 0.01 , 0.01 , 0.095, 0.   , 0.01 ])\n",
      "***** Version: 0.0.4 tol**-3 *****\n",
      "     fun: 0.925042662375372\n",
      "     jac: array([1.16 , 1.006, 2.629, 1.903, 1.217, 1.497, 1.589, 3.839, 1.388,\n",
      "       1.75 , 1.381, 1.313, 1.407, 2.497, 1.162, 1.57 , 1.062, 2.791,\n",
      "       2.493, 1.909, 1.671, 1.58 , 2.331, 0.858, 1.232, 1.513, 1.137,\n",
      "       2.517, 1.214, 1.813, 1.731, 2.457, 1.603, 1.421, 1.351, 1.486,\n",
      "       1.296, 1.494, 1.525, 2.146, 1.109, 1.877, 2.037, 1.596, 2.369,\n",
      "       1.498, 2.143, 2.24 , 2.014, 2.   , 1.518, 1.65 , 2.229, 1.423,\n",
      "       0.902, 1.698, 1.26 , 1.561, 2.429, 2.75 , 1.549, 1.156, 2.238])\n",
      " message: 'Optimization terminated successfully'\n",
      "    nfev: 128\n",
      "     nit: 2\n",
      "    njev: 2\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([0.  , 0.  , 0.01, 0.01, 0.  , 0.01, 0.01, 0.1 , 0.01, 0.01, 0.  ,\n",
      "       0.  , 0.  , 0.01, 0.  , 0.  , 0.  , 0.  , 0.01, 0.01, 0.01, 0.01,\n",
      "       0.01, 0.1 , 0.  , 0.  , 0.  , 0.01, 0.  , 0.  , 0.  , 0.01, 0.  ,\n",
      "       0.  , 0.  , 0.  , 0.  , 0.01, 0.1 , 0.01, 0.1 , 0.01, 0.01, 0.01,\n",
      "       0.01, 0.1 , 0.1 , 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.  , 0.  ,\n",
      "       0.01, 0.  , 0.  , 0.01, 0.01, 0.1 , 0.  , 0.01])\n",
      "***** Version: 0.0.4 tol**-3 *****\n",
      "     fun: 0.9537013219437566\n",
      "     jac: array([1.197, 1.036, 2.701, 1.955, 1.252, 1.543, 1.637, 3.947, 1.431,\n",
      "       1.813, 1.423, 1.35 , 1.451, 2.567, 1.201, 1.619, 1.096, 2.873,\n",
      "       2.563, 1.961, 1.723, 1.629, 2.412, 0.887, 1.27 , 1.556, 1.174,\n",
      "       2.588, 1.246, 1.87 , 1.786, 2.528, 1.643, 1.457, 1.391, 1.525,\n",
      "       1.336, 1.537, 1.578, 2.21 , 1.141, 1.932, 2.098, 1.64 , 2.424,\n",
      "       1.552, 2.211, 2.303, 2.075, 2.06 , 1.572, 1.707, 2.298, 1.458,\n",
      "       0.93 , 1.749, 1.3  , 1.607, 2.499, 2.833, 1.595, 1.189, 2.301])\n",
      " message: 'Optimization terminated successfully'\n",
      "    nfev: 139\n",
      "     nit: 2\n",
      "    njev: 2\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([0.  , 0.  , 0.01, 0.01, 0.  , 0.01, 0.01, 0.1 , 0.01, 0.01, 0.  ,\n",
      "       0.  , 0.  , 0.01, 0.  , 0.  , 0.  , 0.  , 0.01, 0.01, 0.01, 0.01,\n",
      "       0.01, 0.1 , 0.  , 0.  , 0.  , 0.01, 0.  , 0.  , 0.  , 0.01, 0.  ,\n",
      "       0.  , 0.  , 0.  , 0.  , 0.01, 0.1 , 0.01, 0.1 , 0.01, 0.01, 0.01,\n",
      "       0.01, 0.1 , 0.1 , 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.  , 0.  ,\n",
      "       0.01, 0.  , 0.  , 0.01, 0.01, 0.1 , 0.  , 0.01])\n",
      "***** Version: 0.0.4 tol**-3 *****\n",
      "     fun: 0.7990970373109522\n",
      "     jac: array([1.388, 1.417, 3.233, 2.231, 1.365, 1.711, 1.78 , 0.786, 1.642,\n",
      "       2.114, 1.501, 1.524, 1.806, 3.017, 1.333, 1.647, 1.362, 3.375,\n",
      "       2.938, 2.347, 1.95 , 1.892, 2.616, 1.09 , 1.547, 1.888, 1.406,\n",
      "       3.031, 1.515, 1.575, 1.541, 3.036, 2.031, 1.733, 1.581, 1.702,\n",
      "       1.44 , 1.681, 1.395, 2.669, 1.176, 2.208, 2.329, 2.045, 2.852,\n",
      "       1.335, 1.497, 2.581, 2.315, 2.109, 1.588, 1.713, 2.114, 1.983,\n",
      "       1.492, 1.783, 1.533, 1.916, 2.629, 2.542, 1.316, 1.391, 2.596])\n",
      " message: 'Optimization terminated successfully'\n",
      "    nfev: 202\n",
      "     nit: 3\n",
      "    njev: 3\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([0.   , 0.035, 0.01 , 0.01 , 0.   , 0.01 , 0.01 , 0.   , 0.01 ,\n",
      "       0.01 , 0.   , 0.   , 0.   , 0.01 , 0.016, 0.   , 0.049, 0.   ,\n",
      "       0.01 , 0.01 , 0.01 , 0.01 , 0.01 , 0.1  , 0.   , 0.   , 0.   ,\n",
      "       0.01 , 0.   , 0.   , 0.   , 0.01 , 0.   , 0.   , 0.   , 0.   ,\n",
      "       0.   , 0.01 , 0.1  , 0.01 , 0.1  , 0.01 , 0.01 , 0.01 , 0.01 ,\n",
      "       0.1  , 0.   , 0.01 , 0.01 , 0.01 , 0.01 , 0.01 , 0.01 , 0.   ,\n",
      "       0.1  , 0.01 , 0.   , 0.   , 0.01 , 0.01 , 0.1  , 0.   , 0.01 ])\n",
      "***** Version: 0.0.4 tol**-3 *****\n",
      "     fun: 0.8233766476645105\n",
      "     jac: array([1.428, 1.466, 3.32 , 2.29 , 1.402, 1.758, 1.829, 0.797, 1.689,\n",
      "       2.185, 1.545, 1.566, 1.859, 3.099, 1.365, 1.694, 1.402, 3.472,\n",
      "       3.017, 2.409, 2.006, 1.946, 2.7  , 1.122, 1.591, 1.94 , 1.448,\n",
      "       3.113, 1.555, 1.62 , 1.586, 3.122, 2.083, 1.778, 1.625, 1.746,\n",
      "       1.48 , 1.726, 1.439, 2.744, 1.209, 2.272, 2.398, 2.098, 2.918,\n",
      "       1.38 , 1.541, 2.649, 2.38 , 2.168, 1.64 , 1.768, 2.179, 2.033,\n",
      "       1.536, 1.834, 1.577, 1.97 , 2.7  , 2.613, 1.353, 1.428, 2.668])\n",
      " message: 'Optimization terminated successfully'\n",
      "    nfev: 203\n",
      "     nit: 4\n",
      "    njev: 3\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([0.   , 0.038, 0.01 , 0.01 , 0.   , 0.01 , 0.01 , 0.   , 0.01 ,\n",
      "       0.01 , 0.   , 0.   , 0.   , 0.01 , 0.013, 0.   , 0.049, 0.   ,\n",
      "       0.01 , 0.01 , 0.01 , 0.01 , 0.01 , 0.1  , 0.   , 0.   , 0.   ,\n",
      "       0.01 , 0.   , 0.   , 0.   , 0.01 , 0.   , 0.   , 0.   , 0.   ,\n",
      "       0.   , 0.01 , 0.1  , 0.01 , 0.1  , 0.01 , 0.01 , 0.01 , 0.01 ,\n",
      "       0.1  , 0.   , 0.01 , 0.01 , 0.01 , 0.01 , 0.01 , 0.01 , 0.   ,\n",
      "       0.1  , 0.01 , 0.   , 0.   , 0.01 , 0.01 , 0.1  , 0.   , 0.01 ])\n",
      "***** Version: 0.0.4 tol**-3 *****\n",
      "     fun: 1.0396772997539665\n",
      "     jac: array([1.309, 1.123, 2.918, 2.109, 1.356, 1.678, 1.778, 4.273, 1.561,\n",
      "       2.004, 1.55 , 1.463, 1.582, 2.778, 1.318, 1.765, 1.196, 3.119,\n",
      "       2.773, 2.118, 1.879, 1.775, 2.654, 0.975, 1.385, 1.686, 1.284,\n",
      "       2.8  , 1.342, 2.041, 1.95 , 2.741, 1.763, 1.567, 1.511, 1.644,\n",
      "       1.454, 1.667, 1.737, 2.402, 1.234, 2.098, 2.283, 1.772, 2.586,\n",
      "       1.715, 2.414, 2.494, 2.258, 2.241, 1.733, 1.877, 2.508, 1.563,\n",
      "       1.015, 1.902, 1.421, 1.746, 2.71 , 3.081, 1.732, 1.287, 2.49 ])\n",
      " message: 'Optimization terminated successfully'\n",
      "    nfev: 128\n",
      "     nit: 2\n",
      "    njev: 2\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([0.  , 0.  , 0.01, 0.01, 0.  , 0.01, 0.01, 0.1 , 0.01, 0.01, 0.  ,\n",
      "       0.  , 0.  , 0.01, 0.  , 0.  , 0.  , 0.  , 0.01, 0.01, 0.01, 0.01,\n",
      "       0.01, 0.1 , 0.  , 0.  , 0.  , 0.01, 0.  , 0.  , 0.  , 0.01, 0.  ,\n",
      "       0.  , 0.  , 0.  , 0.  , 0.01, 0.1 , 0.01, 0.1 , 0.01, 0.01, 0.01,\n",
      "       0.01, 0.1 , 0.1 , 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.  , 0.  ,\n",
      "       0.01, 0.  , 0.  , 0.01, 0.01, 0.1 , 0.  , 0.01])\n",
      "***** Version: 0.0.4 tol**-3 *****\n",
      "     fun: 0.8628876234812849\n",
      "     jac: array([1.493, 1.494, 3.445, 2.402, 1.494, 1.875, 1.951, 1.513, 1.785,\n",
      "       2.256, 1.584, 1.623, 1.924, 3.248, 1.488, 1.816, 1.482, 3.648,\n",
      "       3.2  , 2.524, 2.174, 2.088, 2.873, 1.198, 1.683, 2.016, 1.553,\n",
      "       3.281, 1.603, 1.804, 1.754, 3.24 , 2.124, 1.843, 1.692, 1.834,\n",
      "       1.574, 1.834, 1.472, 2.872, 1.25 , 2.37 , 2.513, 2.213, 3.003,\n",
      "       1.416, 1.599, 2.816, 2.519, 2.277, 1.777, 1.913, 2.332, 2.084,\n",
      "       1.485, 1.955, 1.683, 2.037, 2.861, 2.844, 1.494, 1.49 , 2.812])\n",
      " message: 'Optimization terminated successfully'\n",
      "    nfev: 387\n",
      "     nit: 6\n",
      "    njev: 6\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([0.002, 0.036, 0.01 , 0.01 , 0.015, 0.01 , 0.01 , 0.019, 0.01 ,\n",
      "       0.01 , 0.   , 0.   , 0.   , 0.01 , 0.03 , 0.   , 0.045, 0.   ,\n",
      "       0.01 , 0.01 , 0.01 , 0.01 , 0.01 , 0.1  , 0.   , 0.   , 0.   ,\n",
      "       0.01 , 0.   , 0.   , 0.   , 0.01 , 0.   , 0.   , 0.   , 0.   ,\n",
      "       0.   , 0.01 , 0.088, 0.01 , 0.1  , 0.01 , 0.01 , 0.01 , 0.01 ,\n",
      "       0.097, 0.   , 0.01 , 0.01 , 0.01 , 0.01 , 0.01 , 0.01 , 0.   ,\n",
      "       0.067, 0.01 , 0.   , 0.   , 0.01 , 0.01 , 0.097, 0.002, 0.01 ])\n",
      "***** Version: 0.0.4 tol**-3 *****\n",
      "     fun: 0.8865475762816873\n",
      "     jac: array([1.532, 1.535, 3.534, 2.461, 1.535, 1.926, 2.004, 1.547, 1.834,\n",
      "       2.325, 1.624, 1.665, 1.975, 3.332, 1.527, 1.866, 1.521, 3.747,\n",
      "       3.283, 2.589, 2.236, 2.146, 2.96 , 1.232, 1.728, 2.068, 1.596,\n",
      "       3.366, 1.64 , 1.858, 1.807, 3.328, 2.173, 1.887, 1.735, 1.88 ,\n",
      "       1.616, 1.882, 1.509, 2.948, 1.282, 2.433, 2.581, 2.269, 3.069,\n",
      "       1.454, 1.639, 2.888, 2.585, 2.336, 1.833, 1.972, 2.398, 2.132,\n",
      "       1.522, 2.007, 1.729, 2.089, 2.934, 2.924, 1.539, 1.529, 2.885])\n",
      " message: 'Optimization terminated successfully'\n",
      "    nfev: 387\n",
      "     nit: 6\n",
      "    njev: 6\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([0.002, 0.037, 0.01 , 0.01 , 0.017, 0.01 , 0.01 , 0.019, 0.01 ,\n",
      "       0.01 , 0.   , 0.   , 0.   , 0.01 , 0.03 , 0.   , 0.045, 0.   ,\n",
      "       0.01 , 0.01 , 0.01 , 0.01 , 0.01 , 0.1  , 0.   , 0.   , 0.   ,\n",
      "       0.01 , 0.   , 0.   , 0.   , 0.01 , 0.   , 0.   , 0.   , 0.   ,\n",
      "       0.   , 0.01 , 0.087, 0.01 , 0.1  , 0.01 , 0.01 , 0.01 , 0.01 ,\n",
      "       0.096, 0.   , 0.01 , 0.01 , 0.01 , 0.01 , 0.01 , 0.01 , 0.   ,\n",
      "       0.066, 0.01 , 0.   , 0.   , 0.01 , 0.01 , 0.098, 0.002, 0.01 ])\n",
      "***** Version: 0.0.4 tol**-3 *****\n",
      "     fun: 1.1256532777889359\n",
      "     jac: array([1.421, 1.211, 3.136, 2.263, 1.46 , 1.814, 1.92 , 4.599, 1.692,\n",
      "       2.194, 1.677, 1.575, 1.713, 2.988, 1.435, 1.911, 1.296, 3.365,\n",
      "       2.982, 2.276, 2.035, 1.922, 2.897, 1.062, 1.5  , 1.817, 1.395,\n",
      "       3.012, 1.438, 2.212, 2.115, 2.955, 1.882, 1.677, 1.631, 1.763,\n",
      "       1.572, 1.797, 1.896, 2.594, 1.328, 2.264, 2.468, 1.904, 2.748,\n",
      "       1.878, 2.618, 2.685, 2.441, 2.423, 1.894, 2.047, 2.717, 1.668,\n",
      "       1.101, 2.056, 1.541, 1.884, 2.922, 3.33 , 1.869, 1.385, 2.68 ])\n",
      " message: 'Optimization terminated successfully'\n",
      "    nfev: 128\n",
      "     nit: 2\n",
      "    njev: 2\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([0.  , 0.  , 0.01, 0.01, 0.  , 0.01, 0.01, 0.1 , 0.01, 0.01, 0.  ,\n",
      "       0.  , 0.  , 0.01, 0.  , 0.  , 0.  , 0.  , 0.01, 0.01, 0.01, 0.01,\n",
      "       0.01, 0.1 , 0.  , 0.  , 0.  , 0.01, 0.  , 0.  , 0.  , 0.01, 0.  ,\n",
      "       0.  , 0.  , 0.  , 0.  , 0.01, 0.1 , 0.01, 0.1 , 0.01, 0.01, 0.01,\n",
      "       0.01, 0.1 , 0.1 , 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.  , 0.  ,\n",
      "       0.01, 0.  , 0.  , 0.01, 0.01, 0.1 , 0.  , 0.01])\n",
      "***** Version: 0.0.4 tol**-3 *****\n",
      "     fun: 0.9337160554531174\n",
      "     jac: array([1.613, 1.617, 3.71 , 2.583, 1.618, 2.029, 2.11 , 1.621, 1.934,\n",
      "       2.463, 1.705, 1.748, 2.08 , 3.503, 1.609, 1.967, 1.6  , 3.945,\n",
      "       3.452, 2.72 , 2.361, 2.265, 3.135, 1.302, 1.818, 2.173, 1.683,\n",
      "       3.538, 1.718, 1.962, 1.909, 3.501, 2.271, 1.977, 1.825, 1.973,\n",
      "       1.703, 1.98 , 1.583, 3.103, 1.344, 2.56 , 2.718, 2.384, 3.203,\n",
      "       1.528, 1.721, 3.036, 2.72 , 2.456, 1.944, 2.089, 2.528, 2.23 ,\n",
      "       1.6  , 2.113, 1.824, 2.195, 3.082, 3.079, 1.622, 1.608, 3.034])\n",
      " message: 'Optimization terminated successfully'\n",
      "    nfev: 389\n",
      "     nit: 6\n",
      "    njev: 6\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([0.001, 0.036, 0.01 , 0.01 , 0.018, 0.01 , 0.01 , 0.02 , 0.01 ,\n",
      "       0.01 , 0.   , 0.   , 0.   , 0.01 , 0.029, 0.   , 0.046, 0.   ,\n",
      "       0.01 , 0.01 , 0.01 , 0.01 , 0.01 , 0.1  , 0.   , 0.   , 0.   ,\n",
      "       0.01 , 0.   , 0.   , 0.   , 0.01 , 0.   , 0.   , 0.   , 0.   ,\n",
      "       0.   , 0.01 , 0.088, 0.01 , 0.1  , 0.01 , 0.01 , 0.01 , 0.01 ,\n",
      "       0.096, 0.   , 0.01 , 0.01 , 0.01 , 0.01 , 0.01 , 0.01 , 0.   ,\n",
      "       0.068, 0.01 , 0.   , 0.   , 0.01 , 0.01 , 0.094, 0.003, 0.01 ])\n",
      "***** Version: 0.0.4 tol**-3 *****\n",
      "     fun: 0.9573225092251346\n",
      "     jac: array([1.652, 1.657, 3.796, 2.643, 1.66 , 2.079, 2.161, 1.659, 1.982,\n",
      "       2.532, 1.745, 1.789, 2.132, 3.586, 1.648, 2.016, 1.64 , 4.041,\n",
      "       3.534, 2.784, 2.422, 2.322, 3.222, 1.336, 1.864, 2.225, 1.726,\n",
      "       3.623, 1.757, 2.013, 1.959, 3.586, 2.32 , 2.022, 1.869, 2.018,\n",
      "       1.745, 2.027, 1.623, 3.179, 1.375, 2.623, 2.786, 2.44 , 3.269,\n",
      "       1.567, 1.763, 3.108, 2.786, 2.516, 1.999, 2.147, 2.593, 2.278,\n",
      "       1.639, 2.165, 1.87 , 2.248, 3.156, 3.155, 1.663, 1.647, 3.107])\n",
      " message: 'Optimization terminated successfully'\n",
      "    nfev: 389\n",
      "     nit: 6\n",
      "    njev: 6\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([0.001, 0.036, 0.01 , 0.01 , 0.019, 0.01 , 0.01 , 0.02 , 0.01 ,\n",
      "       0.01 , 0.   , 0.   , 0.   , 0.01 , 0.028, 0.   , 0.046, 0.   ,\n",
      "       0.01 , 0.01 , 0.01 , 0.01 , 0.01 , 0.1  , 0.   , 0.   , 0.   ,\n",
      "       0.01 , 0.   , 0.   , 0.   , 0.01 , 0.   , 0.   , 0.   , 0.   ,\n",
      "       0.   , 0.01 , 0.088, 0.01 , 0.1  , 0.01 , 0.01 , 0.01 , 0.01 ,\n",
      "       0.096, 0.   , 0.01 , 0.01 , 0.01 , 0.01 , 0.01 , 0.01 , 0.   ,\n",
      "       0.068, 0.01 , 0.   , 0.   , 0.01 , 0.01 , 0.094, 0.004, 0.01 ])\n",
      "***** Version: 0.0.4 tol**-3 *****\n",
      "     fun: 0.9936962782884938\n",
      "     jac: array([1.714, 1.809, 3.932, 2.706, 1.664, 2.092, 2.172, 0.874, 2.018,\n",
      "       2.679, 1.857, 1.862, 2.227, 3.667, 1.599, 2.029, 1.678, 4.156,\n",
      "       3.571, 2.841, 2.395, 2.322, 3.291, 1.35 , 1.898, 2.305, 1.742,\n",
      "       3.687, 1.832, 1.942, 1.906, 3.728, 2.448, 2.093, 1.932, 2.048,\n",
      "       1.76 , 2.036, 1.751, 3.265, 1.438, 2.717, 2.876, 2.472, 3.383,\n",
      "       1.699, 1.859, 3.128, 2.836, 2.586, 2.006, 2.156, 2.635, 2.385,\n",
      "       1.843, 2.195, 1.887, 2.348, 3.197, 3.116, 1.616, 1.693, 3.167])\n",
      " message: 'Optimization terminated successfully'\n",
      "    nfev: 203\n",
      "     nit: 3\n",
      "    njev: 3\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([0.   , 0.051, 0.01 , 0.01 , 0.   , 0.01 , 0.01 , 0.   , 0.01 ,\n",
      "       0.01 , 0.   , 0.   , 0.   , 0.01 , 0.   , 0.   , 0.049, 0.   ,\n",
      "       0.01 , 0.01 , 0.01 , 0.01 , 0.01 , 0.1  , 0.   , 0.   , 0.   ,\n",
      "       0.01 , 0.   , 0.   , 0.   , 0.01 , 0.   , 0.   , 0.   , 0.   ,\n",
      "       0.   , 0.01 , 0.1  , 0.01 , 0.1  , 0.01 , 0.01 , 0.01 , 0.01 ,\n",
      "       0.1  , 0.   , 0.01 , 0.01 , 0.01 , 0.01 , 0.01 , 0.01 , 0.   ,\n",
      "       0.1  , 0.01 , 0.   , 0.   , 0.01 , 0.01 , 0.1  , 0.   , 0.01 ])\n",
      "***** Version: 0.0.4 tol**-3 *****\n",
      "     fun: 1.0178700691964409\n",
      "     jac: array([1.755, 1.855, 4.019, 2.764, 1.701, 2.141, 2.221, 0.886, 2.065,\n",
      "       2.749, 1.901, 1.904, 2.279, 3.748, 1.639, 2.077, 1.715, 4.253,\n",
      "       3.65 , 2.903, 2.451, 2.376, 3.375, 1.383, 1.941, 2.356, 1.783,\n",
      "       3.769, 1.871, 1.989, 1.953, 3.814, 2.498, 2.137, 1.976, 2.091,\n",
      "       1.801, 2.081, 1.796, 3.34 , 1.47 , 2.78 , 2.943, 2.526, 3.449,\n",
      "       1.746, 1.905, 3.197, 2.902, 2.646, 2.059, 2.212, 2.699, 2.435,\n",
      "       1.887, 2.246, 1.932, 2.401, 3.269, 3.189, 1.653, 1.731, 3.237])\n",
      " message: 'Optimization terminated successfully'\n",
      "    nfev: 203\n",
      "     nit: 3\n",
      "    njev: 3\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([0.   , 0.052, 0.01 , 0.01 , 0.   , 0.01 , 0.01 , 0.   , 0.01 ,\n",
      "       0.01 , 0.   , 0.   , 0.   , 0.01 , 0.   , 0.   , 0.048, 0.   ,\n",
      "       0.01 , 0.01 , 0.01 , 0.01 , 0.01 , 0.1  , 0.   , 0.   , 0.   ,\n",
      "       0.01 , 0.   , 0.   , 0.   , 0.01 , 0.   , 0.   , 0.   , 0.   ,\n",
      "       0.   , 0.01 , 0.1  , 0.01 , 0.1  , 0.01 , 0.01 , 0.01 , 0.01 ,\n",
      "       0.1  , 0.   , 0.01 , 0.01 , 0.01 , 0.01 , 0.01 , 0.01 , 0.   ,\n",
      "       0.1  , 0.01 , 0.   , 0.   , 0.01 , 0.01 , 0.1  , 0.   , 0.01 ])\n",
      "***** Version: 0.0.4 tol**-3 *****\n",
      "     fun: 1.0280660472661762\n",
      "     jac: array([1.774, 1.777, 4.055, 2.822, 1.781, 2.229, 2.316, 1.772, 2.128,\n",
      "       2.738, 1.869, 1.913, 2.287, 3.834, 1.765, 2.164, 1.758, 4.328,\n",
      "       3.778, 2.974, 2.601, 2.493, 3.481, 1.439, 1.998, 2.381, 1.855,\n",
      "       3.873, 1.872, 2.165, 2.109, 3.841, 2.465, 2.154, 2.002, 2.153,\n",
      "       1.871, 2.169, 1.749, 3.405, 1.47 , 2.81 , 2.989, 2.606, 3.465,\n",
      "       1.687, 1.893, 3.322, 2.985, 2.696, 2.163, 2.319, 2.788, 2.423,\n",
      "       1.756, 2.32 , 2.007, 2.407, 3.376, 3.383, 1.783, 1.765, 3.324])\n",
      " message: 'Optimization terminated successfully'\n",
      "    nfev: 389\n",
      "     nit: 6\n",
      "    njev: 6\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([0.   , 0.037, 0.01 , 0.01 , 0.022, 0.01 , 0.01 , 0.021, 0.01 ,\n",
      "       0.01 , 0.   , 0.   , 0.   , 0.01 , 0.026, 0.   , 0.045, 0.   ,\n",
      "       0.01 , 0.01 , 0.01 , 0.01 , 0.01 , 0.1  , 0.   , 0.   , 0.   ,\n",
      "       0.01 , 0.   , 0.   , 0.   , 0.01 , 0.   , 0.   , 0.   , 0.   ,\n",
      "       0.   , 0.01 , 0.087, 0.01 , 0.1  , 0.01 , 0.01 , 0.01 , 0.01 ,\n",
      "       0.095, 0.   , 0.01 , 0.01 , 0.01 , 0.01 , 0.01 , 0.01 , 0.   ,\n",
      "       0.068, 0.01 , 0.   , 0.   , 0.01 , 0.01 , 0.095, 0.005, 0.01 ])\n",
      "***** Version: 0.0.4 tol**-3 *****\n",
      "     fun: 1.0516257758834542\n",
      "     jac: array([1.815, 1.817, 4.141, 2.881, 1.822, 2.279, 2.368, 1.81 , 2.176,\n",
      "       2.807, 1.91 , 1.954, 2.339, 3.916, 1.803, 2.214, 1.798, 4.425,\n",
      "       3.859, 3.037, 2.661, 2.55 , 3.568, 1.473, 2.043, 2.433, 1.898,\n",
      "       3.956, 1.911, 2.216, 2.159, 3.925, 2.514, 2.198, 2.046, 2.198,\n",
      "       1.913, 2.216, 1.791, 3.48 , 1.501, 2.873, 3.056, 2.661, 3.531,\n",
      "       1.727, 1.936, 3.393, 3.051, 2.756, 2.217, 2.377, 2.853, 2.472,\n",
      "       1.795, 2.372, 2.052, 2.46 , 3.449, 3.459, 1.823, 1.804, 3.397])\n",
      " message: 'Optimization terminated successfully'\n",
      "    nfev: 389\n",
      "     nit: 6\n",
      "    njev: 6\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([0.   , 0.037, 0.01 , 0.01 , 0.023, 0.01 , 0.01 , 0.021, 0.01 ,\n",
      "       0.01 , 0.   , 0.   , 0.   , 0.01 , 0.026, 0.   , 0.045, 0.   ,\n",
      "       0.01 , 0.01 , 0.01 , 0.01 , 0.01 , 0.1  , 0.   , 0.   , 0.   ,\n",
      "       0.01 , 0.   , 0.   , 0.   , 0.01 , 0.   , 0.   , 0.   , 0.   ,\n",
      "       0.   , 0.01 , 0.087, 0.01 , 0.1  , 0.01 , 0.01 , 0.01 , 0.01 ,\n",
      "       0.094, 0.   , 0.01 , 0.01 , 0.01 , 0.01 , 0.01 , 0.01 , 0.   ,\n",
      "       0.067, 0.01 , 0.   , 0.   , 0.01 , 0.01 , 0.095, 0.005, 0.01 ])\n",
      "***** Version: 0.0.4 tol**-3 *****\n",
      "     fun: 1.0751740410367625\n",
      "     jac: array([1.856, 1.858, 4.228, 2.941, 1.863, 2.33 , 2.42 , 1.849, 2.225,\n",
      "       2.875, 1.952, 1.995, 2.391, 4.   , 1.843, 2.264, 1.838, 4.522,\n",
      "       3.941, 3.102, 2.721, 2.608, 3.654, 1.507, 2.088, 2.486, 1.941,\n",
      "       4.041, 1.95 , 2.266, 2.208, 4.01 , 2.563, 2.243, 2.09 , 2.243,\n",
      "       1.955, 2.264, 1.831, 3.556, 1.532, 2.936, 3.124, 2.718, 3.597,\n",
      "       1.766, 1.978, 3.465, 3.118, 2.816, 2.272, 2.435, 2.918, 2.52 ,\n",
      "       1.835, 2.425, 2.099, 2.513, 3.522, 3.535, 1.862, 1.843, 3.47 ])\n",
      " message: 'Optimization terminated successfully'\n",
      "    nfev: 389\n",
      "     nit: 6\n",
      "    njev: 6\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([0.   , 0.037, 0.01 , 0.01 , 0.023, 0.01 , 0.01 , 0.021, 0.01 ,\n",
      "       0.01 , 0.   , 0.   , 0.   , 0.01 , 0.025, 0.   , 0.045, 0.   ,\n",
      "       0.01 , 0.01 , 0.01 , 0.01 , 0.01 , 0.1  , 0.   , 0.   , 0.   ,\n",
      "       0.01 , 0.   , 0.   , 0.   , 0.01 , 0.   , 0.   , 0.   , 0.   ,\n",
      "       0.   , 0.01 , 0.086, 0.01 , 0.1  , 0.01 , 0.01 , 0.01 , 0.01 ,\n",
      "       0.094, 0.   , 0.01 , 0.01 , 0.01 , 0.01 , 0.01 , 0.01 , 0.   ,\n",
      "       0.067, 0.01 , 0.   , 0.   , 0.01 , 0.01 , 0.095, 0.006, 0.01 ])\n",
      "***** Version: 0.0.4 tol**-3 *****\n",
      "     fun: 1.0987138780582972\n",
      "     jac: array([1.897, 1.898, 4.314, 3.001, 1.904, 2.38 , 2.472, 1.888, 2.274,\n",
      "       2.944, 1.993, 2.036, 2.443, 4.084, 1.883, 2.314, 1.878, 4.619,\n",
      "       4.024, 3.166, 2.782, 2.666, 3.741, 1.542, 2.133, 2.538, 1.985,\n",
      "       4.125, 1.989, 2.317, 2.258, 4.095, 2.611, 2.288, 2.135, 2.288,\n",
      "       1.998, 2.312, 1.871, 3.632, 1.563, 2.999, 3.192, 2.774, 3.663,\n",
      "       1.806, 2.02 , 3.538, 3.185, 2.876, 2.327, 2.493, 2.982, 2.569,\n",
      "       1.875, 2.478, 2.146, 2.566, 3.596, 3.61 , 1.902, 1.882, 3.543])\n",
      " message: 'Optimization terminated successfully'\n",
      "    nfev: 389\n",
      "     nit: 6\n",
      "    njev: 6\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([0.   , 0.037, 0.01 , 0.01 , 0.024, 0.01 , 0.01 , 0.021, 0.01 ,\n",
      "       0.01 , 0.   , 0.   , 0.   , 0.01 , 0.025, 0.   , 0.044, 0.   ,\n",
      "       0.01 , 0.01 , 0.01 , 0.01 , 0.01 , 0.1  , 0.   , 0.   , 0.   ,\n",
      "       0.01 , 0.   , 0.   , 0.   , 0.01 , 0.   , 0.   , 0.   , 0.   ,\n",
      "       0.   , 0.01 , 0.086, 0.01 , 0.1  , 0.01 , 0.01 , 0.01 , 0.01 ,\n",
      "       0.094, 0.   , 0.01 , 0.01 , 0.01 , 0.01 , 0.01 , 0.01 , 0.   ,\n",
      "       0.067, 0.01 , 0.   , 0.   , 0.01 , 0.01 , 0.095, 0.006, 0.01 ])\n",
      "***** Version: 0.0.4 tol**-3 *****\n",
      "     fun: 1.138904778554071\n",
      "     jac: array([1.961, 2.087, 4.454, 3.057, 1.886, 2.383, 2.469, 0.945, 2.304,\n",
      "       3.1  , 2.123, 2.114, 2.541, 4.152, 1.836, 2.318, 1.897, 4.738,\n",
      "       4.044, 3.21 , 2.73 , 2.646, 3.796, 1.545, 2.157, 2.613, 1.989,\n",
      "       4.176, 2.065, 2.226, 2.186, 4.244, 2.75 , 2.354, 2.197, 2.308,\n",
      "       2.005, 2.309, 2.024, 3.716, 1.628, 3.092, 3.278, 2.794, 3.779,\n",
      "       1.98 , 2.139, 3.541, 3.229, 2.949, 2.325, 2.492, 3.022, 2.681,\n",
      "       2.107, 2.503, 2.157, 2.67 , 3.626, 3.556, 1.84 , 1.92 , 3.587])\n",
      " message: 'Optimization terminated successfully'\n",
      "    nfev: 201\n",
      "     nit: 3\n",
      "    njev: 3\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([0.   , 0.057, 0.01 , 0.01 , 0.   , 0.01 , 0.01 , 0.   , 0.01 ,\n",
      "       0.01 , 0.   , 0.   , 0.   , 0.01 , 0.   , 0.   , 0.043, 0.   ,\n",
      "       0.01 , 0.01 , 0.01 , 0.01 , 0.01 , 0.1  , 0.   , 0.   , 0.   ,\n",
      "       0.01 , 0.   , 0.   , 0.   , 0.01 , 0.   , 0.   , 0.   , 0.   ,\n",
      "       0.   , 0.01 , 0.1  , 0.01 , 0.1  , 0.01 , 0.01 , 0.01 , 0.01 ,\n",
      "       0.1  , 0.   , 0.01 , 0.01 , 0.01 , 0.01 , 0.01 , 0.01 , 0.   ,\n",
      "       0.1  , 0.01 , 0.   , 0.   , 0.01 , 0.01 , 0.1  , 0.   , 0.01 ])\n",
      "Optimal weights of the efficient set portfolios\n",
      ": [[0.045 0.    0.01  ... 0.    0.    0.01 ]\n",
      " [0.05  0.    0.01  ... 0.    0.    0.01 ]\n",
      " [0.052 0.    0.01  ... 0.    0.    0.01 ]\n",
      " ...\n",
      " [0.    0.037 0.01  ... 0.095 0.006 0.01 ]\n",
      " [0.    0.037 0.01  ... 0.095 0.006 0.01 ]\n",
      " [0.    0.057 0.01  ... 0.1   0.    0.01 ]]\n",
      "\n",
      "Annualized Risk and Return of the efficient set portfolios:\n",
      " [[19.186 22.655]\n",
      " [19.138 22.632]\n",
      " [18.994 22.422]\n",
      " [18.834 22.143]\n",
      " [18.71  21.882]\n",
      " [18.443 20.963]\n",
      " [18.053 19.368]\n",
      " [18.524 20.11 ]\n",
      " [17.562 16.912]\n",
      " [17.417 15.985]\n",
      " [17.322 15.279]\n",
      " [17.275 14.865]\n",
      " [17.247 14.597]\n",
      " [17.233 14.434]\n",
      " [17.179 13.776]\n",
      " [17.875 15.211]\n",
      " [17.09  12.42 ]\n",
      " [17.088 12.343]\n",
      " [17.719 11.961]\n",
      " [17.024 11.294]\n",
      " [17.021 11.323]\n",
      " [17.011 11.011]\n",
      " [18.214  7.679]\n",
      " [17.093 12.061]\n",
      " [16.989 10.389]\n",
      " [16.988 10.323]\n",
      " [16.989 10.359]\n",
      " [18.827  5.194]\n",
      " [18.827  5.194]\n",
      " [16.992 10.33 ]\n",
      " [18.827  5.194]\n",
      " [18.827  5.194]\n",
      " [16.971  9.554]\n",
      " [18.827  5.194]\n",
      " [18.827  5.194]\n",
      " [17.062 10.705]\n",
      " [17.063 10.595]\n",
      " [18.827  5.194]\n",
      " [16.96   8.435]\n",
      " [16.959  8.269]\n",
      " [18.827  5.194]\n",
      " [16.959  8.242]\n",
      " [16.959  8.114]\n",
      " [17.078 10.023]\n",
      " [17.079 10.002]\n",
      " [16.958  7.8  ]\n",
      " [16.958  7.71 ]\n",
      " [16.958  7.623]\n",
      " [16.958  7.54 ]\n",
      " [17.085  9.895]]\n"
     ]
    }
   ],
   "source": [
    "from utils import getVolatilityArr, SLSQPSSolver\n",
    "\n",
    "# resultado do beta obtido em outro arquivo, mas que denota quais stocks podemos considerar mais ou menos volateis\n",
    "new_beta = [0.782, 0.704, 1.753, 1.257, 0.733, 1.166, 1.173, 0.558, 1.016, 1.150, 0.729, 0.788, 0.988, 1.712, 0.676, 0.960, 0.637, 1.939, 1.643, 1.280, 1.183, 1.114, 1.481, 0.546, 0.822, 0.959, 0.760, 1.642, 0.749, 0.964, 0.943, 1.651, 0.985,\\\n",
    "            0.912, 0.821, 0.942, 0.936, 1.113, 0.367, 1.486, 0.416, 1.226, 1.307, 1.180, 1.582, 0.168, 0.543, 1.393, 1.252, 1.055, 1.197, 1.229, 1.219, 0.948, 0.678, 1.007, 0.794, 0.946, 1.418, 1.419, 0.802, 0.720, 1.444]\n",
    "# volatArr eh a \"classificacao\" das stocks entre volateis ou nao\n",
    "[volatArr, boundariesArr] = getVolatilityArr(new_beta,[0.1, 0.01])\n",
    "\n",
    "# numero de stocks possiveis no portifolio\n",
    "portiSize = 63\n",
    "\n",
    "# init\n",
    "optimalPorti = []\n",
    "minRiskPoint = []\n",
    "expPortifolioReturnPoint = []\n",
    "\n",
    "# simulando 100 pontos variÃ¡veis de risco\n",
    "for points in range(0,50):\n",
    "    riskAversParam = points/50.0\n",
    "    result = SLSQPSSolver(meanReturns, covReturns, riskAversParam, portiSize, volatArr, boundariesArr)\n",
    "    optimalPorti.append(result.x) # x eh o resultado da otimizacao\n",
    "    \n",
    "xOptiArr = np.array(optimalPorti)\n",
    "minRiskPointArr = np.diagonal(np.matmul((np.matmul(xOptiArr,covReturns)), np.transpose(xOptiArr)))\n",
    "\n",
    "riskPoint = np.sqrt(minRiskPointArr*251)\n",
    "expPortfolioReturnPoint= np.matmul(xOptiArr, meanReturns)\n",
    "retPoint = 251 * np.array(expPortfolioReturnPoint)\n",
    "\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "\n",
    "print(\"Optimal weights of the efficient set portfolios\\n:\", xOptiArr)\n",
    "print(\"\\nAnnualized Risk and Return of the efficient set portfolios:\\n\",\\\n",
    "      np.c_[riskPoint, retPoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "b2763972-a0b7-4c04-ac9d-22e74a102e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_risk = np.min(riskPoint)\n",
    "max_return = np.max(retPoint)\n",
    "mininum_risk_index = np.where(riskPoint == np.min(riskPoint))\n",
    "max_return_index = np.where(max_return == np.max(retPoint))\n",
    "\n",
    "# Portifolio de risco minimo\n",
    "menorRisco = riskPoint[mininum_risk_index[0][0]]\n",
    "retornoRiscoMinimo = retPoint[mininum_risk_index[0][0]]\n",
    "\n",
    "portifolio_final_minrisk = []\n",
    "for i in range(len(stockLabels)):\n",
    "    if xOptiArr[mininum_risk_index[0][0]][i] >= 0.009:\n",
    "        portifolio_final_minrisk.append([stockLabels[i], xOptiArr[mininum_risk_index[0][0]][i]])\n",
    "\n",
    "# Portifolio de maior retorno\n",
    "riscoMaiorRetorno = riskPoint[max_return_index[0][0]]\n",
    "maximoRetorno = retPoint[max_return_index[0][0]]\n",
    "\n",
    "portifolio_final_maxreturn = []\n",
    "for i in range(len(stockLabels)):\n",
    "    if xOptiArr[max_return_index[0][0]][i] >= 0.009:\n",
    "        portifolio_final_maxreturn.append([stockLabels[i], xOptiArr[max_return_index[0][0]][i]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "64f28a7f-c004-4e85-a36c-e71309684abd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47\n",
      "16.95824431642454\n",
      "***** Melhores PortifÃ³lios *****\n",
      "Menor Risco: 16.96% de risco e 7.62% de retorno\n",
      "Carteira: \n",
      " [['ASAI3.SA', 0.03737128962813604], ['AZUL4.SA', 0.009999999893781482], ['B3SA3.SA', 0.009999999999995835], ['BBSE3.SA', 0.02335727790601589], ['BBDC4.SA', 0.009999999999997667], ['BBDC3.SA', 0.00999999999999908], ['BRAP4.SA', 0.021104361147870457], ['BBAS3.SA', 0.009999999999998212], ['BRKM5.SA', 0.00999999999999557], ['COGN3.SA', 0.009999999999983303], ['CPLE6.SA', 0.025419536169517382], ['CPFE3.SA', 0.044594099217057134], ['CYRE3.SA', 0.00999999999998371], ['ECOR3.SA', 0.009999999999995984], ['ELET3.SA', 0.009999999999998831], ['ELET6.SA', 0.01], ['EMBR3.SA', 0.00999999999998502], ['ENBR3.SA', 0.09999999999999278], ['EZTC3.SA', 0.009999999999982233], ['GOLL4.SA', 0.009999999999983225], ['ITUB4.SA', 0.009999999999997452], ['JBSS3.SA', 0.0864552551003534], ['JHSF3.SA', 0.009999999999988697], ['KLBN11.SA', 0.0999999999999948], ['RENT3.SA', 0.009999999999996144], ['LCAM3.SA', 0.009999999999993032], ['LREN3.SA', 0.009999999999999308], ['MGLU3.SA', 0.009999999999987928], ['MRFG3.SA', 0.09413553690486232], ['MRVE3.SA', 0.009999999999988466], ['MULT3.SA', 0.009999999999993283], ['PCAR3.SA', 0.00999999999999425], ['PETR3.SA', 0.009999999999997662], ['PETR4.SA', 0.00999999999999848], ['PRIO3.SA', 0.00999999999999029], ['RADL3.SA', 0.06730391561657552], ['RAIL3.SA', 0.009999999999999258], ['UGPA3.SA', 0.009999999999986556], ['USIM5.SA', 0.009999999999980315], ['VALE3.SA', 0.0945612641692967]]\n",
      "----------------------------------------------------------------------------------------------\n",
      "Maior Retorno: 19.19% de risco e 22.65% de retorno\n",
      "Carteira: \n",
      " [['ABEV3.SA', 0.045097102363424506], ['AZUL4.SA', 0.009999999999999681], ['B3SA3.SA', 0.009999999999999763], ['BBDC4.SA', 0.009999999999999301], ['BBDC3.SA', 0.0099999999999994], ['BBAS3.SA', 0.01], ['BRKM5.SA', 0.01], ['COGN3.SA', 0.009999999999999837], ['CPLE6.SA', 0.1], ['CVCB3.SA', 0.01], ['CYRE3.SA', 0.009999999999999874], ['ECOR3.SA', 0.00999999999999986], ['ELET3.SA', 0.009999999999999938], ['ELET6.SA', 0.01], ['EMBR3.SA', 0.009999999999999674], ['ENBR3.SA', 0.1], ['EQTL3.SA', 0.05190816916633184], ['EZTC3.SA', 0.009999999999999952], ['GGBR4.SA', 0.032626097367800685], ['GOAU4.SA', 0.05004447958059062], ['GOLL4.SA', 0.009999999999999858], ['ITUB4.SA', 0.009999999999999816], ['JBSS3.SA', 0.1], ['JHSF3.SA', 0.01], ['RENT3.SA', 0.01], ['LCAM3.SA', 0.009999999999999802], ['LREN3.SA', 0.009999999999999684], ['MRFG3.SA', 0.1], ['BEEF3.SA', 0.09132473507339989], ['MRVE3.SA', 0.01], ['MULT3.SA', 0.009999999999999667], ['PCAR3.SA', 0.009999999999999518], ['PETR3.SA', 0.01], ['PETR4.SA', 0.01], ['PRIO3.SA', 0.009999999999999992], ['RAIL3.SA', 0.01], ['SBSP3.SA', 0.028999416448454215], ['UGPA3.SA', 0.01], ['USIM5.SA', 0.01]]\n"
     ]
    }
   ],
   "source": [
    "print(mininum_risk_index[0][0])\n",
    "print(riskPoint[48])\n",
    "\n",
    "print(\"***** Melhores PortifÃ³lios *****\")\n",
    "print(f'Menor Risco: {menorRisco:.2f}% de risco e {retornoRiscoMinimo:.2f}% de retorno')\n",
    "print('Carteira: \\n', portifolio_final_minrisk)\n",
    "print('----------------------------------------------------------------------------------------------')\n",
    "print(f'Maior Retorno: {riscoMaiorRetorno:.2f}% de risco e {maximoRetorno:.2f}% de retorno')\n",
    "print('Carteira: \\n',portifolio_final_maxreturn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "7ebc0ccb-1b09-463b-b923-4921827e04f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# print(np.c_[riskPoint, retPoint]*100)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 10), dpi=80)\n",
    "plt.scatter(riskPoint, retPoint)\n",
    "plt.title('Efficient Frontier')\n",
    "plt.xlabel('Risco da carteira (%)')\n",
    "plt.ylabel('Retorno esperado (%)')\n",
    "plt.scatter(riscoMaiorRetorno,maximoRetorno,s=100)\n",
    "plt.scatter(menorRisco, retornoRiscoMinimo, s=100)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
